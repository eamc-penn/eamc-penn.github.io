[
  {
    "objectID": "src/ml.html",
    "href": "src/ml.html",
    "title": "Introduction to Machine Learning",
    "section": "",
    "text": "Describe what an algorithm is and how they are used in both clinical medicine and everyday life.\nDescribe what it means to learn and how learning applies to machine learning.\nIdentify key applications of machine learning and when computational tools can be helpful (and potentially harmful) for patient care.\nMade with ❤ by the EAMC Team ©2024."
  },
  {
    "objectID": "src/ml.html#learning-objectives",
    "href": "src/ml.html#learning-objectives",
    "title": "Introduction to Machine Learning",
    "section": "",
    "text": "Describe what an algorithm is and how they are used in both clinical medicine and everyday life.\nDescribe what it means to learn and how learning applies to machine learning.\nIdentify key applications of machine learning and when computational tools can be helpful (and potentially harmful) for patient care."
  },
  {
    "objectID": "src/ml.html#food-for-thought",
    "href": "src/ml.html#food-for-thought",
    "title": "Introduction to Machine Learning",
    "section": "Food for Thought",
    "text": "Food for Thought\n\nAre there tasks in healthcare for which automated methods–such as computer algorithms and artificial intelligence (AI)–should never be used for? Why or why not?\n\n\n\n What if automated methods perform the task on par with humans? What if they perform better than humans?\n\nDoes understanding how automated methods arrive at their predictions change any of your answers to question 1?"
  },
  {
    "objectID": "src/ml.html#introduction-to-machine-learning",
    "href": "src/ml.html#introduction-to-machine-learning",
    "title": "Introduction to Machine Learning",
    "section": "Introduction to Machine Learning",
    "text": "Introduction to Machine Learning\n\nWhat is an algorithm?\nAn algorithm is any function that computes an output from an input. We already use algorithms in everyday life and in clinical medicine. For example, here is an algorithm that you might use to determine when to walk to JMEC based on 3 different variables:\ny = some_algorithm_for_when_to_walk_to_jmec(\n    how_long_does_it_typically_take_to_walk_to_campus,\n    how_much_sleep_did_I_get_last_night,\n    is_class_mandatory\n)\nwhere y is when you decide to walk to campus.\nSome algorithms can be written down exactly. For example, compute the anion gap given patient values, or compute the MAP of a patient given their blood pressure.\nOther algorithms are harder to express on paper. For example, how to run a code or how to determine whether to admit a patient or not.\nComputers can run algorithms that can be written down exactly. But how can we teach them how to run algorithms that are hard to express? To answer this question, let’s reflect on how we as students learn algorithms that might be hard to express.\n\n\nLearning by Observing\nComputers can learn by observation, much like how medical students learn! Consider some of the following scenarios:\n\nA Database of Genomes\nDuring your clinical research year, your advisor gives you a large dataset of many different patient genomes. We can denote this dataset of \\(n\\) patients as the variable \\(\\mathcal{D}_n\\). By analyzing \\(\\mathcal{D}_n\\), we try to gain insights into which genes make individuals unique, and which ones all patients share in common.\n\n\nA Randomized Control Trial\nYour research mentor is impressed with your analysis and gives you a new project: investigating if a new drug abastatin lowers patient cholesterol levels. He gives you a large dataset of anonymized patient data containing two variables: whether the patient was given abastatin or placebo (\\(x\\)), and whether the patient had a reduction in their cholesterol levels (\\(y\\)). By analyzing this dataset, we try to learn whether or not abastatin is an effective drug for hypercholesterolemia.\n\n\nA Patient with Sepsis\n\n\n For those of you with a machine learning background, A Database of Genomes is an unsupervised learning problem, A Randomized Control Trial is a supervised learning problem, and A Patient with Sepsis is a reinforcement learning problem. You can learn more about each of these types of machine learning problems here!\nA 52 year-old male presents with acute-onset altered mental status and fever. Vitals are notable for BP 90/60 and T 103.4. We can denote the patient as a variable \\(x\\) consisting of all of the relevant attributes of the patient: their HPI, past medical history, current lab values and vitals, etc.\nOn our first day as a medical student, we might not know what to do with this patient. Do we admit them and start them on IV antibiotics? Do we call a neurology consult? Do we just send the patient home? Each of these clinical interventions can be thought of as an action \\(a\\) that we can take to try to help the patient get better.\nAfter observing a patient \\(x\\) and performing an action \\(a\\), we monitor the patient to see if they improve. The patient’s outcome can be denoted as a variable \\(y\\) (for example, \\(y=0\\) if the patient deteriorates and \\(y=1\\) if the patient gets better). We observe the clinical outcome \\(y\\), and use it to learn a better algorithm so that next time we see a similar patient, we can take a better action that leads to a more favorable outcome.\nOver the course of medical school, we see hundreds (if not thousands) of tuples \\((x, a, y)\\) through clerkships, sub-Is, exams, and UWorld, and use this dataset of patient-outcome pairs to learn hard-to-write-down algorithms for choosing the best clinical intervention \\(a\\) given a patient \\(x\\) to maximize the outcome \\(y\\). In other words, we learn by observation.\n\n\n\nWhat does it mean to “learn”?\nNo patient is exactly identical to any other patient, including the patients that you learn from. If all you can do is regurgitate the dataset you learned from, this is not learning! Put simply…\n\n\n\n\n\n\nLearning = Generalization\n\n\n\nAfter observing the many different patient cases and outcomes, we want to be able to generalize to new patients in the future, such that we know what to do as clinicians for future, previously unknown patients.\n\n\n\n\nMachine Learning as a Framework\nMachine learning (ML) uses the exact same framework of learning through observation to learn hard-to-write-down algorithms from data as exact steps that a computer can execute.\n\n\n Just like how we all have different mnemonics and mental maps on how to approach clinical reasoning, the exact steps in the algorithm that ML learns may not be the same as the steps that clinicians learn! This is an important problem that researchers are still trying to solve.\nThe fundamental goal of machine learning is to learn hard-to-write-down algorithms from past observations to hopefully make accurate predictions for future observations.\n\n\n\n\n\n\nWhat problems might cause algorithms to generalize poorly to new patients?\n\n\n\n\n\nThere are a number of reasons. Here are a couple:\n\nNew patients are very different from the patients used to train the algorithm. TODO\nThe algorithm had too many inputs and learned relationships between inputs and outputs that are not true. For example, if we try to predict the weather based on features like TODO\n\nWhat other problems did you think of? Are there any ways to fix the problems that we’ve identified?\n\n\n\n\n\nWhen can machine learning be a helpful tool?\nConsider the following example cases. Would you want to use machine learning in each of these cases?\n\n\n\n\n\n\n1. Given patient lab values and health record data, ML predicts the age of a patient.\n\n\n\n\n\nNo, it’s easy to just look up the age of a patient from the patient chart.\n\n\n\n\n\n\n\n\n\n2. Given patient blood pressure values, ML predicts the patient’s MAP.\n\n\n\n\n\nNo, computing MAP is an easy-to-write-down algorithm.\n\n\n\n\n\n\n\n\n\n3. Given patient lab values, imaging data, genomic data, and other attributes, ML predicts whether the patient is at risk for a cancer with no known cure.\n\n\n\n\n\nNo, even if ML derives an algorithm for this task, there is nothing actionable that we can do about it.\n\n\n\n\n\n\n\n\n\n4. Given patient bowel sound recordings, ML predicts the probability a patient has an SBO.\n\n\n\n\n\nNo, we don’t have any datasets of patient bowel sounds mapping to the presence/absence of an SBO, so there are no prior observations for ML to learn from.\n\n\n\n\n\n\n\n\n\n5. Given patient lab values, ML predicts the probability a patient has ribose-5-phosphate isomerase (RPI) deficiency, the second rarest disease in the world.\n\n\n\n\n\nNo, we don’t have nearly enough observations of patients with RPI deficiency. Even if we did have enough data to learn this algorithm, we also would rarely/never even need to use this algorithm.\n\n\n\nIn summary, ML is useful for tasks that are\n\nhard-to-write-down;\nassociated with a lot of prior observations; and\ncan lead to actionable utility for patients and/or clinicians by automating hard, repetitive, and/or common tasks.\n\n\n\nEvidence-Based Medicine Discussion\nShould AI be used to improve access to mental health resources?\n\nOverview Article: Stade EC, Stirman SW, Ungar LH, Boland CL, Schwartz HA, Yaden DB, Sedoc J, DeRubeis RJ, Willer R, Eichstaedt JC. Large language models could change the future of behavioral healthcare: A proposal for responsible development and evaluation. npj Mental Health Res 3(12). (2024). doi: 10.1038/s44184-024-00056-z. PMID: 38609507\nYes, AI is more empathetic than physicians: Ayers JW, Poliak A, Dredze M, Leas EC, Zhu Z, Kelley J, Faux DJ, Goodman AM, Longhurst CA, Hogarth M, Smith DM. Comparing physician and artificial intelligence chatbot responses to patient questions posted to a public social media forum. JAMA Intern Med 183(6): 589-96. (2023). doi: 10.1001/jamainternmed.2023.1838. PMID: 37115527\nNo, AI is too slow to appropriately escalate mental health risk scenarios: Heston TF. Safety of large language models in addressing depression. Cureus 15(12): e50729. (2023). doi: 10.7759/cureus.50729. PMID: 38111813"
  },
  {
    "objectID": "src/ml.html#hands-on-tutorial",
    "href": "src/ml.html#hands-on-tutorial",
    "title": "Introduction to Machine Learning",
    "section": "Hands-On Tutorial",
    "text": "Hands-On Tutorial\nLet’s explore how state-of-the-art AI models currently perform as mental health resources for real-world patients. Here’s an example of a chatbot that’s currently available for anyone to use on the Internet.\nAssume the role of a patient seeking mental health support and resources. How accurate is the model as a therapist? How empathetic is the model? Would you use this particular model for mental health support? Why or why not?\n\n\nThis particular model is hosted on Hugging Face, which has become the de-facto website for publishing publicly available machine learning models like the one we’re exploring here. Anyone can download a model and use it for applications such as mental health among others."
  },
  {
    "objectID": "src/ml.html#summary",
    "href": "src/ml.html#summary",
    "title": "Introduction to Machine Learning",
    "section": "Summary",
    "text": "Summary\nAlgorithms are functions that map inputs to outputs. Some algorithms are easy to describe while others are harder to write down. Machine learning is the process of computers learning hard-to-write-down algorithms from past observations, with the goal of learning algorithms that are generalizable to new sets of inputs."
  },
  {
    "objectID": "src/ml.html#additional-readings",
    "href": "src/ml.html#additional-readings",
    "title": "Introduction to Machine Learning",
    "section": "Additional Readings",
    "text": "Additional Readings\n\nTopol EJ. High-performance medicine: The convergence of human and artificial intelligence. Nat Med 25: 44-56. (2019). doi: 10.1038/s41591-018-0300-7. PMID: 30617339\nSidey-Gibbons JAM, Sidey-Gibbons CJ. Machine learning in medicine: A practical introduction. BMC Medical Research Methodology 19(64). (2019). doi: 10.1186/s12874-019-0681-4. PMID: 30890124\nJAMA Podcast with Dr. Kevin Johnson from Penn Medicine. October 4, 2023. Link."
  },
  {
    "objectID": "src/index.html",
    "href": "src/index.html",
    "title": "Ethical Algorithms for the Modern Clinician",
    "section": "",
    "text": "Ethical Algorithms for the Modern Clinician is a short course intended to teach future clinicians the basics of machine learning (ML) and artificial intelligence (AI) as they pertain to clinical practice.\nIn contrast to other tutorials available online, this resource is not intended to teach how machine learning algorithms work or how to build novel models. Instead, we introduce ML from clinical practitioner’s perspective and discuss what ML does right, where it falls short, and how it will impact patient care. This short course focuses on five key aspects:\n\nIntroduction to Machine Learning: What is machine learning? How is it similar to and different from conventional software?\nAlgorithmic Bias: How can algorithms like machine learning be biased against certain patient sub-populations? How can we quantify, detect, and reduce bias in clinical decision making algorithms?\nAnonymization: How can we effectively anonymize patient data? Why does anonymization often fall short in protecting patient identities?\nPatient Privacy: How do machine learning algorithms preserve or break patient privacy? How can we ensure that clinicians use algorithms responsibly while maintaining patient privacy?\nGenerative AI: What is generative AI, and how might it be used in the clinic? What are the new challenges and opportunities associated with generative AI models?\n\nJust as epidemiology was introduced in foundational component medical school curricula alongside the rise of evidence-based guidelines1, we firmly believe that it is crucial for future clinicians to have a working understanding of machine learning algorithms as they become increasingly adapted in clinical practice. The material presented herein is intended to be foundational knowledge for medical students of all backgrounds without assuming any prior experience with machine learning or algorithmic ethics.1 Fowkes FG, Gehlbach SH, Farrow SC, et al. Epidemiology for medical students: A course relevant to clinical practice. Int J Epidemiol 13(4): 538-41. (1984). doi: 10.1093/ije/13.4.538. PMID: 6519897\nMade with ❤ by the EAMC Team ©2024."
  },
  {
    "objectID": "src/bias.html",
    "href": "src/bias.html",
    "title": "Algorithmic Bias",
    "section": "",
    "text": "Define algorithmic bias and recognize that bias is often a subjective property of an algorithm.\nReflect on important case studies demonstrating the real-world impact of bias.\nDescribe potential bias mitigation strategies and how we can incorporate them into clinical decision making.\nMade with ❤ by the EAMC Team ©2024."
  },
  {
    "objectID": "src/bias.html#learning-objectives",
    "href": "src/bias.html#learning-objectives",
    "title": "Algorithmic Bias",
    "section": "",
    "text": "Define algorithmic bias and recognize that bias is often a subjective property of an algorithm.\nReflect on important case studies demonstrating the real-world impact of bias.\nDescribe potential bias mitigation strategies and how we can incorporate them into clinical decision making."
  },
  {
    "objectID": "src/bias.html#what-is-bias",
    "href": "src/bias.html#what-is-bias",
    "title": "Algorithmic Bias",
    "section": "What is Bias?",
    "text": "What is Bias?\nBias is a term that is often used broadly but has a very precise definition. Bias is always defined with respect to two related concepts: (1) protected attribute(s), and (2) a definition of harm.\n\nA protected attribute is an attribute about a patient that we want to ensure there is no bias against. Examples of protected attributes might be patient age, gender, or ethnicity.\nA definition of harm is how we choose to define when bias is present. Common definitions of harm include an algorithm’s overall error rate, or its false positive or false negative rate.\n\nWhen we choose the protected attribute and a definition of harm, we can then define when an algorithm is biased. Namely, an algorithm is biased if it causes an increase in harm for a subpopulation of patients with respect to the protected attribute(s). For example, if we define the protected attribute as a patient’s race and the definition of harm as the algorithm’s error rate, then the algorithm is biased if its error rate is higher for Black Americans than White Americans."
  },
  {
    "objectID": "src/bias.html#key-terminology-binary-classification",
    "href": "src/bias.html#key-terminology-binary-classification",
    "title": "Algorithmic Bias",
    "section": "Key Terminology: Binary Classification",
    "text": "Key Terminology: Binary Classification\nIn this section, we primarily focus on understanding algorithmic bias as they pertain to binary classifiers, which are algorithms that seek to classify patients into one of two categories. Binary classification is common in clinical medicine: we often want to classify patients by…\n\ndiagnosing them with a disease or not;\nidentifying if they are sick or not; or\ndeciding whether a patient can be discharged or not.\n\nIn each of these situations, our goal is to label an input patient using a positive label or a negative label.\nAs with any clinical task, algorithms rarely get 100% accuracy. We may mistakenly diagnose a patient as having a disease when they in fact do not have it, or decide that a patient can be discharged when they need to stay in the hospital for additional treatment. These are situations where an algorithm might cause harm to a patient.\n\nAn algorithm’s false positive rate (FPR) is the rate at which an algorithm falsely assigns the positive label to a patient (i.e., telling a patient AMAB1 that they are pregnant).\nAn algorithm’s false negative rate (FNR) is the rate at which an algorithm falsely assigns the negative label to a patient (i.e., telling a patient that is 8 months pregnant that they are not pregnant).\nAn algorithm’s overall error rate is the rate at which an algorithm assigns the wrong label to a patient.\n\n1 Assigned male at birth, meaning the patient has XY sex chromosomes.In general, we want to minimize an algorithm’s FPR, FNR, and overall error rate. FPR, FNR, and overall error are commonly used definitions of harm in assessing for algorithmic bias.\nIt is important to recognize that the consequences of false positives and false negatives can be different depending on the task. For example, in colon cancer screening, a false negative (e.g., missing a precancerous polyp on a colonoscopy) is much worse than a false positive (e.g., taking out a potential polyp that turns out not to be precancerous). This means that the consequence of a false negative is much more significant than that of a false positive for colon cancer screening."
  },
  {
    "objectID": "src/bias.html#sources-of-bias",
    "href": "src/bias.html#sources-of-bias",
    "title": "Algorithmic Bias",
    "section": "Sources of Bias",
    "text": "Sources of Bias\nWhat causes an algorithm to be potentially biased? Bias can be due to a wide variety of reasons, including population-dependent discrepancies in…\n\n\n Other than the sources listed below, what are some other potential causes of bias that algorithms might suffer from?\n\nAvailability of Data\nEspecially for machine learning algorithms, it is important for models to be trained on diverse datasets from many different patient populations. If the dataset used to train a model is composed of 90% White patients and only 10% Black patients, then the resulting algorithm will likely perform inaccurately on Black patients.\nThis is a common problem not just for machine learning algorithms, but also in insights from randomized control trials! For example, take a look at the 2023 ISCHEMIA Trial2 from the American Heart Association. According to Supplementary Table 1, approximately 77% of the patients in the study were male. Would you trust the insights from the trial for your female patients?2 Hochman JS, Anthopolos R, Reynolds HR, et al. Survival after invasive or conservative management of stable coronary disease. Circulation 147(1): 8-19. (2022). doi: 10.1161/CIRCULATIONA HA.122.062714. PMID: 36335918\n\n\nPathophysiology\nDifferent patient populations may have different underlying mechanisms of disease, and so lumping patients together using a single predictive algorithm may limit that algorithm’s ability to represent all the different mechanisms of disease.\n\n\nQuality of Data\nSuppose we have two CT scanners in the hospital: Scanner 1 and Scanner 2. Scanner 1 was made in 1970 and Scanner 2 was made in 2020; as a result, Scanner 1 produces very low-quality, low-resolution images compared to Scanner 2. If we learn an algorithm to diagnose a disease from CT scans, then the algorithm will likely perform worse on input scans from Scanner 1. This is because lower quality scans contain less information about the patient, and so patients imaged with Scanner 1 will be inherently less predictable.\n\n\nHow Data is Acquired\nThe data that we choose to collect to learn an algorithm can also introduce biases. For example, suppose you are conducting a study investigating the relationship between number of leadership positions and success in medical school for medical students. Focusing only on leadership positions might result in algorithms that are biased against students from lower socioeconomic backgrounds who can succeed in medical school but may have to focus on things such as taking care of loved ones or part-time employment that was not factored into the initial algorithm design. In summary, it is important to be thoughtful about not only algorithms, but also datasets as potential sources of bias!\n\n\n How might collecting too many data features bias algorithms?"
  },
  {
    "objectID": "src/bias.html#case-studies",
    "href": "src/bias.html#case-studies",
    "title": "Algorithmic Bias",
    "section": "Case Studies",
    "text": "Case Studies\nTo better understand the sources of algorithmic bias and why they are important, let’s look at some commonly cited case studies (both clinical and non-clinical):\n\nPulmonary Function Testing (PFT)\nPFTs are used in clinical practice to evaluate lung health. The patient’s measured lung values are compared with the expected lung values given the patient’s age, height, sex assigned at birth, and ethnicity among other factors.\nResearchers have found that using a patient’s race as input into the expected lung value calculation can result in different PFT results, with implications for access to certain disease treatments and disability benefits. However, they also report that using race has also allowed patients to benefit from treatment options that they would have otherwise not had access to based on societal guidelines.\nThe 2019 American Thoracic Society (ATS) guidelines currently offer both race-specific and race-neutral algorithms, leaving it up to the discretion of the provider to determine how PFTs are used in clinical practice. Other historical examples of bias in clinical medicine include eGFR calculations, opioid risk mitigation, and care assessment evaluation as functions of race and other patient demographic information. To learn more, check out the Health Equity article from List et al.33 List JM, Palevsky P, Tamang S, et al. Eliminating algorithmic racial bias in clinical decision support algorithms: Use cases from the Veterans Health Administration. Health Equity 7(1): 809-16. (2023). doi: 10.1089/heq.2023.0037. PMID: 38076213\n\n\nCOMPAS/ProPublica\nIn 1998, a private company built the COMPAS algorithm, which is an algorithm that takes in information about arrested/incarcerated individuals and returns a prediction of how likely the individual will commit future crimes. Inputs into the COMPAS algorithm include individual demographics, criminal history, personal and family history, and the nature of the charged crime among others.\nThe COMPAS algorithm is used in the real world to help the judiciary system set bonds and evaluate arrested individuals. High (low) COMPAS scores mean more (less) likely to commit future crimes.\nProPublica is a nonprofit journalism organization that conducted an independent evaluation of the COMPAS tool in 2016. Their main finding was that COMPAS is biased.\n\nThe distribution of COMPAS risk scores skews low for white individuals but more uniform for black individuals. In other words, white individuals were more often “let off the hook” than black individuals for the same crime.\nLooking at historical data, the false positive rate (FPR) was significantly higher for black individuals than white individuals. In other words, COMPAS was more likely to incorrectly predict that a black individual would commit a future crime.\n\n\n\n\n\n\n\nDoes this mean that the COMPAS algorithm is racially biased (using the risk score as the definition of harm)? What are some potentially other reasons why white scores may be lower?\n\n\n\n\n\nOther reasons might include…\n\nthe nature of the subjective questions asked about the individual’s personal and family history;\nother causal variables like upbringing and socioeconomic status that might be racially correlated; and\nthe number of black individuals used in the development of the COMPAS algorithm.\n\nWhat other potential reasons did you think of?\n\n\n\nCOMPAS Developer Response to the ProPublica report was that COMPAS is not biased.\n\nArea under the receiver operating curve (AUROC), a metric of classifier “goodness,” for black and white subpopulations are equal. Therefore, COMPAS is not biased and we can make sure the FPR of both populations are equal by setting different classifier thresholds for the two subpopulations.\n\n\n\nImage Generation Using Google Gemini\nIn late 2023, Google introduced a new generative AI model called Gemini, which is able to complete a variety of tasks such as generating images from input text descriptions. Gemini was released to the public, and users quickly found that Gemini stood out from prior generative models because it was able to generate more diverse sets of images, such as producing images of people of color when prompted for an “American woman” or producing images images of women when prompted for historically male-dominated roles, such as a lawyer or an engineer. This was seen as a major step forward in tackling the bias associated with other generative models.44 Nicoletti L and Bass D. Humans are biased. Generative AI is even worse. Bloomberg. (2023). Link to article\nHowever, an unintended side effect was that the model also generated historically inaccurate images when prompted for images of “1943 German soldiers” or “US senators from the 1800s.” In these settings, it would be inaccurate to generate images of people of color given these input prompts."
  },
  {
    "objectID": "src/bias.html#how-can-we-reduce-bias",
    "href": "src/bias.html#how-can-we-reduce-bias",
    "title": "Algorithmic Bias",
    "section": "How can we reduce bias?",
    "text": "How can we reduce bias?\nThe most common reason why bias occurs in machine learning is that we train models to be accurate on the “average” patient. In other words, if there are more patients from one subpopulation than another in the dataset used to learn an algorithm, then the algorithm will almost certainly be more accurate on the majority population. Naively learning algorithms in this fashion will result in accurate but biased models.\nOn the other hand, we could instead implement a completely random algorithm - for example, deciding whether a patient should be admitted or not solely based on the flip of a coin. Such an algorithm would be completely unbiased, but not very accurate.\nThese two examples demonstrate that in general, there is a tradeoff between accuracy and bias. As we train models to be more accurate, they often become more biased at the same time. Researchers are currently working on ways to overcome these limitations5, but this is an incredibly common empirical finding that we see in practice.5 Chouldechova A, Roth A. The frontiers of fairness in machine learning. arXiv Preprint. (2018). doi: 10.48550/arXiv.1810.08810\n\nFairness is not Compositional\nWhy is training both fair and accurate models hard? There are a lot of complex answers to this question, but one important reason is that fairness is not compositional. In simpler terms, imagine that we have a screening tool that seeks to predict whether a patient has a disease with 50% prevalence in the population. The algorithm has been shown to be unbiased against patient gender (male versus female) with regards to sensitivity, and also unbiased against patient color (“blue” versus “green” people) using the same metric. Does this mean that the algorithm is also unbiased against blue female people?\nNo! The screening tool can still be biased against blue female people in this toy case. Here’s an illustrative diagram of one possibility:\n\n\n\nFigure 1: Fairness Gerrymandering. Adapted from Kearns M, Neel S, Roth A, Wu ZS. Preventing fairness gerrymandering: Auditing and learning for subgroup fairness. Proc Int Conf Mach Learn 80: 2564-72. (2018). Link to Paper\n\n\nIn Figure 1, we can see that our screening tool is “fair” with respect to gender and color by only accepting individuals that are either both blue and male or both green and female. Ensuring that models are fair for certain subgroups doesn’t mean that those models are also fair for members of the intersections of those groups, or other entirely unrelated subgroups. In other words, fairness conditions do not compose. This is why fairness is such a hard problem to tackle!\n\n\nWhat can we do about this as clinicians?\nThe most important thing to help reduce bias is recognize that all real-world algorithms are biased! How algorithms are biased and to what extent depend on your definition of harm and the patient attribute(s) that you’re focusing on. These definitions inherently differ between persons and scenarios. Recognizing our own biases in the algorithms used by both computers and humans is critical so that we make the best decisions for each individual patient."
  },
  {
    "objectID": "src/bias.html#hands-on-tutorial",
    "href": "src/bias.html#hands-on-tutorial",
    "title": "Algorithmic Bias",
    "section": "Hands-On Tutorial",
    "text": "Hands-On Tutorial\nTo better understand how bias can impact algorithms, let’s take a look at a simple example of a binary classifier algorithm that seeks to predict whether a loan applicant will either (1) pay back the loan or (2) default on the loan.\n\n\n\n\n\n\nWill a loan applicant pay back the loan?\n\n\n\n\n\n\nGo to this study from Wattenberg et al. at Google. Read through the article.\nPlay around with setting different thresholds for the algorithm to better understand the tradeoffs between the different metrics, such as accuracy, positive rate, and profit.\nSimulate different loan decisions and different thresholds for two different groups: the blue and orange subpopulations. What happens when we use different conditions to set the different thresholds, like maximizing the bank’s profit, using a group-unaware strategy, and ensuring equal opportunity?\n\n\n\n\nIn the above example, we looked at distributing loans. However, we can imagine a similar, more clinically relevant scenario: our own clinical algorithms for determining if a patient needs supplemental oxygen. For simplicity, let’s assume that this decision is solely based on the patient’s O2 saturation. Here are two potential strategies we can use:\n\n\n\n\n\n\nDoes a patient need supplemental oxygen?\n\n\n\n\n\nImportant Context: O2 saturation measurements are less accurate for Black individuals.6 More specifically, pulse oximeters often overestimate true O2 saturation for Black patients.\nTwo Strategies to Consider:\n\nGroup-Unaware Strategy: For all patients irregardless of skin color, we only start supplemental oxygen if SpO2 &lt; 92%.\nEqual Opportunity Strategy: The same fraction of Black and White patients should be on SpO2, so we will use the SpO2 &lt; 92% cutoff for White patients and use a separate SpO2 &lt; 94% cutoff for Black patients.\n\nWhich strategy is more biased? Is using race as an input into algorithms (as in the equal opportunity strategy) “good”? Why or why not?\n\n\n\n6 Al-Halawani R, Charlton PH, Qassem M, et al. A review of the effect of skin pigmentation on pulse oximeter accuracy. Physiol Meas 44(5): 05TR01. (2023). doi: 10.1088/1361-6579/acd51a. PMID: 37172609\n How would your answer to this question change (or not) if instead we were deciding whether a patient was a lung transplant candidate? How about if we were deciding whether to start losartan therapy for newly diagnosed hypertension?"
  },
  {
    "objectID": "src/bias.html#discussion-questions",
    "href": "src/bias.html#discussion-questions",
    "title": "Algorithmic Bias",
    "section": "Discussion Questions",
    "text": "Discussion Questions\n\nWho do you agree with: ProPublica or the COMPAS developers? In other words, do you believe that the COMPAS algorithm is biased based on the evidence presented? Would you be comfortable having it used to determine the outcomes of the judicial system for close friends or family?\nIn the hands-on tutorial, we explored how even simple binary classifiers can be biased in scenarios such as determining loan repayment and supplemental oxygen requirements. What other analogous “binary classifier” clinical situations have you encountered? How did you decide your strategy on how to set your own “threshold” for positive and negative labels? Did your strategy vary between different patients?"
  },
  {
    "objectID": "src/bias.html#summary",
    "href": "src/bias.html#summary",
    "title": "Algorithmic Bias",
    "section": "Summary",
    "text": "Summary\nBias is defined based on (1) protected attribute(s) and (2) a definition of harm. Because our definition of harm can vary from person-to-person, bias is often subjective. The impact of bias depends on the clinical scenario and the real-world implications of the definition of harm. It is important to recognize our own internal sources of bias in addition to the biases of clinical and computational algorithms."
  },
  {
    "objectID": "src/bias.html#additional-readings",
    "href": "src/bias.html#additional-readings",
    "title": "Algorithmic Bias",
    "section": "Additional Readings",
    "text": "Additional Readings\n\nNicoletti L and Bass D. Humans are biased. Generative AI is even worse. Bloomberg. (2023). Link to article\nKearns M, Roth A. Responsible AI in the wild: Lessons learned at AWS. Amazon Science Blog. (2023). Link to article\nList JM, Palevsky P, Tamang S, et al. Eliminating algorithmic racial bias in clinical decision support algorithms: Use cases from the Veterans Health Administration. Health Equity 7(1): 809-16. (2023). doi: 10.1089/heq.2023.0037. PMID: 38076213 Mittermaier M, Raza MM, Kvedar JC. Bias in AI-based models for medical applications: Challenges and mitigation strategies. npj Digit Med 6(113). (2023). doi: 10.1038/s41746-023-00858-z. PMID: 37311802"
  },
  {
    "objectID": "src/changelog.html",
    "href": "src/changelog.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n\n\n\nMade with ❤ by the EAMC Team ©2024."
  },
  {
    "objectID": "src/anonymization.html",
    "href": "src/anonymization.html",
    "title": "Anonymization",
    "section": "",
    "text": "TODO\nTODO\nTODO\nMade with ❤ by the EAMC Team ©2024."
  },
  {
    "objectID": "src/anonymization.html#learning-objectives",
    "href": "src/anonymization.html#learning-objectives",
    "title": "Anonymization",
    "section": "",
    "text": "TODO\nTODO\nTODO"
  },
  {
    "objectID": "src/anonymization.html#overview",
    "href": "src/anonymization.html#overview",
    "title": "Anonymization",
    "section": "Overview",
    "text": "Overview\nAs clinicians, we deal with patient data every day, and have an ethical (and legal) responsibility to protect patient privacy and confidential information. At the same time, we often work alongside scientists to use patient data to advance our understanding of science. How can we gain meaningful insights from data while still protecting patient identity?\nAccording to the Health Insurance Portability and Accountability Act (HIPAA), one way to accomplish this is through data anonymization. In general, there are two main ways that researchers anonymize data:\n\nCoarsening means we decrease the granularity of the features. For example, using 5-digit zip codes may make it too easy to identify individuals from a dataset, so we might instead choose to coarsen the zip codes by removing the last two digits of each zip code. Instead of including the exact ages of patients, we often coarse the data to only include the decade of the age of the patient.\nReduction means we remove entire features altogether. For example, we might choose to remove all patient names and medical record numbers from a dataset before making it accessible to researchers."
  },
  {
    "objectID": "src/anonymization.html#a-detailed-look-hipaa-phi",
    "href": "src/anonymization.html#a-detailed-look-hipaa-phi",
    "title": "Anonymization",
    "section": "A Detailed Look: HIPAA PHI",
    "text": "A Detailed Look: HIPAA PHI\nLet’s take a look at the official list of HIPAA-protected attributes from the Health and Human Services Department:\n\nNames;\nAll geographical subdivisions smaller than a state (e.g., street address, city, county, precinct, ZIP code except for the initial three digits of a ZIP code).1\nAll dates (except year) directly related to an individual (e.g., birth date, admission date, exact ages in years over the ages of 90).2\nPhone numbers, fax numbers, email addresses.\nSocial security numbers, health plan beneficiary numbers, driver license numbers, medical record numbers, etc.\nLicense plates\nIP addresses\nBiometric identifiers (i.e., finger prints, voice recordings, genomic data)\nFull-face photographic images\nAny other unique identifying number, characteristic, or code\n\n1 The initial three digits of a zip code is still considered PHI by HIPAA if the number of individuals residing in all zip codes with those initial three digits is less than 20,000. Why do you think this is the case? How do you think the cutoff of 20,000 individuals was determined?2 Why are ages over 90 years-old considered PHI, but not younger ages?Are there any attributes listed that you didn’t expect? How about attributes that aren’t listed above but should be included?"
  },
  {
    "objectID": "src/anonymization.html#hands-on-tutorial",
    "href": "src/anonymization.html#hands-on-tutorial",
    "title": "Anonymization",
    "section": "Hands-On Tutorial",
    "text": "Hands-On Tutorial\nLet’s take a look at the following table:\nTODO\n\nEvidence-Based Medicine Discussion\nTODO\n\n\nSummary\nTODO\n\n\nAdditional Readings\n\nTODO\nTODO\nTODO\nTODO"
  },
  {
    "objectID": "src/index.html#institutional-partners",
    "href": "src/index.html#institutional-partners",
    "title": "Ethical Algorithms for the Modern Clinician",
    "section": "Institutional Partners",
    "text": "Institutional Partners"
  },
  {
    "objectID": "src/index.html#authors",
    "href": "src/index.html#authors",
    "title": "Ethical Algorithms for the Modern Clinician",
    "section": "Authors",
    "text": "Authors\nMichael Yao is an MD-PhD candidate at Penn, and has formerly worked as an software engineer and researcher at companies such as Microsoft Research, Scale AI, Glass Health, and Hyperfine. His current thesis research is in trustworthiness and robustness for deep learning, offline optimization, meta-learning, and bandit problem formulations. Michael is broadly interested in developing methods that leverage prior knowledge and data to help algorithms better generalize to new distributions.\nAllison Chae is an MD candidate at Penn researching how AI algorithms can be used for real-world clinical workflows. She currently works alongside radiologists to better understand how physicians and computer systems interact with one another. Allison’s research is supported by the AΩA Society and the Perelman School of Medicine."
  },
  {
    "objectID": "src/index.html#expert-collaborators",
    "href": "src/index.html#expert-collaborators",
    "title": "Ethical Algorithms for the Modern Clinician",
    "section": "Expert Collaborators",
    "text": "Expert Collaborators\nEthical Algorithms for the Modern Clinician is backed by experts in both clinical medicine and machine learning.\nTODO"
  },
  {
    "objectID": "src/index.html#how-to-use-this-book",
    "href": "src/index.html#how-to-use-this-book",
    "title": "Ethical Algorithms for the Modern Clinician",
    "section": "How to Use This Book",
    "text": "How to Use This Book\nThere are a number of different ways to learn this book. Some find it most helpful to directly self-study the material, which usually can be done in a weekend according to prior students. We believe the most effective and fruitful way to learn the content herein is in small, discussion-based classroom environments. Many of the discussion questions included in each module have no single right answer, and it is often helpful to hear from peers on how they’re thinking about the material, too. To facilitate classroom implementation, the content has been broken down into five modules lasting approximately one-hour each."
  },
  {
    "objectID": "src/index.html#overview",
    "href": "src/index.html#overview",
    "title": "Ethical Algorithms for the Modern Clinician",
    "section": "",
    "text": "Ethical Algorithms for the Modern Clinician is a short course intended to teach future clinicians the basics of machine learning (ML) and artificial intelligence (AI) as they pertain to clinical practice.\nIn contrast to other tutorials available online, this resource is not intended to teach how machine learning algorithms work or how to build novel models. Instead, we introduce ML from clinical practitioner’s perspective and discuss what ML does right, where it falls short, and how it will impact patient care. This short course focuses on five key aspects:\n\nIntroduction to Machine Learning: What is machine learning? How is it similar to and different from conventional software?\nAlgorithmic Bias: How can algorithms like machine learning be biased against certain patient sub-populations? How can we quantify, detect, and reduce bias in clinical decision making algorithms?\nAnonymization: How can we effectively anonymize patient data? Why does anonymization often fall short in protecting patient identities?\nPatient Privacy: How do machine learning algorithms preserve or break patient privacy? How can we ensure that clinicians use algorithms responsibly while maintaining patient privacy?\nGenerative AI: What is generative AI, and how might it be used in the clinic? What are the new challenges and opportunities associated with generative AI models?\n\nJust as epidemiology was introduced in foundational component medical school curricula alongside the rise of evidence-based guidelines1, we firmly believe that it is crucial for future clinicians to have a working understanding of machine learning algorithms as they become increasingly adapted in clinical practice. The material presented herein is intended to be foundational knowledge for medical students of all backgrounds without assuming any prior experience with machine learning or algorithmic ethics.1 Fowkes FG, Gehlbach SH, Farrow SC, et al. Epidemiology for medical students: A course relevant to clinical practice. Int J Epidemiol 13(4): 538-41. (1984). doi: 10.1093/ije/13.4.538. PMID: 6519897"
  },
  {
    "objectID": "src/index.html#footnotes",
    "href": "src/index.html#footnotes",
    "title": "Ethical Algorithms for the Modern Clinician",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFowkes FG, Gehlbach SH, Farrow SC, et al. Epidemiology for medical students: A course relevant to clinical practice. Int J Epidemiol 13(4): 538-41. (1984). doi: 10.1093/ije/13.4.538. PMID: 6519897↩︎"
  },
  {
    "objectID": "src/index.html#about-the-authors",
    "href": "src/index.html#about-the-authors",
    "title": "Ethical Algorithms for the Modern Clinician",
    "section": "About the Authors",
    "text": "About the Authors\nMichael Yao is an MD-PhD candidate at Penn, and has formerly worked as an software engineer and researcher at companies such as Microsoft Research, Scale AI, Glass Health, and Hyperfine. His current thesis research is in trustworthiness and robustness for deep learning, offline optimization, meta-learning, and bandit problem formulations. Michael is broadly interested in developing methods that leverage prior knowledge and data to help algorithms better generalize to new distributions.\nAllison Chae is an MD candidate at Penn researching how AI algorithms can be used for real-world clinical workflows. She currently works alongside radiologists to better understand how physicians and computer systems interact with one another. Allison’s research is supported by the AΩA Society and the Perelman School of Medicine."
  },
  {
    "objectID": "src/genai.html",
    "href": "src/genai.html",
    "title": "Generative AI",
    "section": "",
    "text": "Define what generative AI and describe its applications in clinical medicine and the life sciences.\nTODO\nTODO\n\nTODO https://www.amazon.science/blog/responsible-ai-in-the-generative-era\nMade with ❤ by the EAMC Team ©2024."
  },
  {
    "objectID": "src/genai.html#learning-objectives",
    "href": "src/genai.html#learning-objectives",
    "title": "Generative AI",
    "section": "",
    "text": "Define what generative AI and describe its applications in clinical medicine and the life sciences.\nTODO\nTODO\n\nTODO https://www.amazon.science/blog/responsible-ai-in-the-generative-era"
  },
  {
    "objectID": "src/privacy.html",
    "href": "src/privacy.html",
    "title": "Patient Privacy",
    "section": "",
    "text": "TODO\nTODO\nTODO\n\nAll of Us https://pubmed.ncbi.nlm.nih.gov/36809550/\nMade with ❤ by the EAMC Team ©2024."
  },
  {
    "objectID": "src/privacy.html#learning-objectives",
    "href": "src/privacy.html#learning-objectives",
    "title": "Patient Privacy",
    "section": "",
    "text": "TODO\nTODO\nTODO\n\nAll of Us https://pubmed.ncbi.nlm.nih.gov/36809550/"
  },
  {
    "objectID": "src/404.html",
    "href": "src/404.html",
    "title": "Page Not Found",
    "section": "",
    "text": "The page you requested cannot be found (perhaps it was moved or renamed).\nYou may want to try searching to find the page’s new location.\n\n\n\n\nMade with ❤ by the EAMC Team ©2024."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Ethical Algorithms for the Modern Clinician",
    "section": "",
    "text": "Ethical Algorithms for the Modern Clinician is a short course intended to teach future clinicians the basics of machine learning (ML) and artificial intelligence (AI) as they pertain to clinical practice.\nThis resource is not intended to teach how machine learning algorithms work or how to build novel models. Instead, we introduce ML from clinical practitioner’s perspective and discuss what ML does right, where it falls short, and how it will impact patient care. This short course focuses on five key aspects:\n\nIntroduction to Machine Learning: What is machine learning? How is it similar to and different from conventional software?\nBias and Fairness: How can algorithms like machine learning be biased against certain patient sub-populations? How can we quantify, detect, and reduce bias in clinical decision making algorithms?\nPrivacy and Anonymization: How can we effectively anonymize patient data? Why does anonymization often fall short in protecting patient identities? How can we ensure that clinicians use algorithms responsibly while maintaining patient privacy?\nInterpretability and Explainability: TODO\nGenerative AI: What is generative AI, and how might it be used in the clinic? What are the new challenges and opportunities associated with generative AI models?\n\nJust as epidemiology was introduced in medical school curricula alongside the rise of evidence-based guidelines1, it is crucial for future clinicians to have a working understanding of machine learning algorithms as they become increasingly adapted in clinical practice. Much of the curriculum has been inspired by the (much longer) ethical algorithms course at Penn taught by Michael Kearns. Ethical Algorithms for the Modern Clinician is intended as foundational knowledge for medical students of all backgrounds - prior experience with machine learning is not required.1 Fowkes FG, Gehlbach SH, Farrow SC, et al. Epidemiology for medical students: A course relevant to clinical practice. Int J Epidemiol 13(4): 538-41. (1984). doi: 10.1093/ije/13.4.538. PMID: 6519897\nMade with ❤ by the EAMC Team ©2024."
  },
  {
    "objectID": "index.html#overview",
    "href": "index.html#overview",
    "title": "Ethical Algorithms for the Modern Clinician",
    "section": "",
    "text": "Ethical Algorithms for the Modern Clinician is a short course intended to teach future clinicians the basics of machine learning (ML) and artificial intelligence (AI) as they pertain to clinical practice.\nThis resource is not intended to teach how machine learning algorithms work or how to build novel models. Instead, we introduce ML from clinical practitioner’s perspective and discuss what ML does right, where it falls short, and how it will impact patient care. This short course focuses on five key aspects:\n\nIntroduction to Machine Learning: What is machine learning? How is it similar to and different from conventional software?\nBias and Fairness: How can algorithms like machine learning be biased against certain patient sub-populations? How can we quantify, detect, and reduce bias in clinical decision making algorithms?\nPrivacy and Anonymization: How can we effectively anonymize patient data? Why does anonymization often fall short in protecting patient identities? How can we ensure that clinicians use algorithms responsibly while maintaining patient privacy?\nInterpretability and Explainability: TODO\nGenerative AI: What is generative AI, and how might it be used in the clinic? What are the new challenges and opportunities associated with generative AI models?\n\nJust as epidemiology was introduced in medical school curricula alongside the rise of evidence-based guidelines1, it is crucial for future clinicians to have a working understanding of machine learning algorithms as they become increasingly adapted in clinical practice. Much of the curriculum has been inspired by the (much longer) ethical algorithms course at Penn taught by Michael Kearns. Ethical Algorithms for the Modern Clinician is intended as foundational knowledge for medical students of all backgrounds - prior experience with machine learning is not required.1 Fowkes FG, Gehlbach SH, Farrow SC, et al. Epidemiology for medical students: A course relevant to clinical practice. Int J Epidemiol 13(4): 538-41. (1984). doi: 10.1093/ije/13.4.538. PMID: 6519897"
  },
  {
    "objectID": "index.html#how-to-use-this-book",
    "href": "index.html#how-to-use-this-book",
    "title": "Ethical Algorithms for the Modern Clinician",
    "section": "How to Use This Book",
    "text": "How to Use This Book\nThere are a number of different ways to learn this book. Some find it most helpful to directly self-study the material, which usually can be done in a weekend according to prior students. We believe the most effective and fruitful way to learn the content herein is in small, discussion-based classroom environments. Many of the discussion questions included in each module have no single right answer, and it is often helpful to hear from peers on how they’re thinking about the material, too. To facilitate classroom implementation, the content has been broken down into five modules lasting approximately one-hour each.\n\n\n\n\n\n\nWhat This Resource Is Not\n\n\n\n\n\nThere are a lot of machine learning resources out there already… what makes Ethical Algorithms for the Modern Clinician different? This resource is not any of the following:\n\nA introduction to theory of and programming for machine learning: If you’re interested in building ML models from scratch, the MedML@Emory Club has put together a fantastic tutorial here, and Andrew Ng also has a great curated list of technical tutorials here.\nAn overview of the computational techniques used to build machine learning models: If you’re looking for something like this, we recommend recent work by Pfob et al. (2022) and Sidey-Gibbons et al. (2019).\nA mathematically rigorous foundation for assessing algorithmic fairness and privacy: If this is something you’re interested in, we highly recommend checking out The Ethical Algorithm and the Ethical Algorithm Design course offered at Penn every year."
  },
  {
    "objectID": "index.html#about-the-authors",
    "href": "index.html#about-the-authors",
    "title": "Ethical Algorithms for the Modern Clinician",
    "section": "About the Authors",
    "text": "About the Authors\nMichael Yao is an MD-PhD candidate at the University of Pennsylvania, and has formerly worked as an software engineer and researcher at companies such as Microsoft Research, Scale AI, Glass Health, and Hyperfine. His current thesis research is in trustworthiness and robustness for deep learning, offline optimization, meta-learning, and bandit problem formulations. Michael is broadly interested in developing methods that leverage prior knowledge and data to help algorithms better generalize to new distributions.\nAllison Chae is an MD candidate at the University of Pennsylvania researching how AI algorithms can be used for real-world clinical workflows. She currently works alongside radiologists to better understand how physicians and computer systems interact with one another. Allison’s research is supported by the AΩA Society and the Perelman School of Medicine."
  },
  {
    "objectID": "index.html#expert-collaborators",
    "href": "index.html#expert-collaborators",
    "title": "Ethical Algorithms for the Modern Clinician",
    "section": "Expert Collaborators",
    "text": "Expert Collaborators\nEthical Algorithms for the Modern Clinician is backed by experts in both clinical medicine and machine learning.\nTODO"
  },
  {
    "objectID": "index.html#institutional-partners",
    "href": "index.html#institutional-partners",
    "title": "Ethical Algorithms for the Modern Clinician",
    "section": "Institutional Partners",
    "text": "Institutional Partners"
  },
  {
    "objectID": "ml.html",
    "href": "ml.html",
    "title": "Introduction to Machine Learning",
    "section": "",
    "text": "Describe what an algorithm is and how they are used in both clinical medicine and everyday life.\nDescribe what it means to learn and how learning applies to machine learning.\nIdentify key applications of machine learning and when computational tools can be helpful (and potentially harmful) for patient care.\nMade with ❤ by the EAMC Team ©2024."
  },
  {
    "objectID": "ml.html#learning-objectives",
    "href": "ml.html#learning-objectives",
    "title": "Introduction to Machine Learning",
    "section": "",
    "text": "Describe what an algorithm is and how they are used in both clinical medicine and everyday life.\nDescribe what it means to learn and how learning applies to machine learning.\nIdentify key applications of machine learning and when computational tools can be helpful (and potentially harmful) for patient care."
  },
  {
    "objectID": "ml.html#food-for-thought",
    "href": "ml.html#food-for-thought",
    "title": "Introduction to Machine Learning",
    "section": "Food for Thought",
    "text": "Food for Thought\n\nAre there tasks in healthcare for which automated methods–such as computer algorithms and artificial intelligence (AI)–should never be used for? Why or why not?\n\n\n\n\n\n\n\nFood for Thought: Are there tasks in healthcare for which automated methods should never be used?\n\n\n\n\n\n What if automated methods perform the task on par with humans? What if they perform better than humans?\n\n\n\n\nDoes understanding how automated methods arrive at their predictions change any of your answers to question 1?"
  },
  {
    "objectID": "ml.html#introduction-to-machine-learning",
    "href": "ml.html#introduction-to-machine-learning",
    "title": "Introduction to Machine Learning",
    "section": "Introduction to Machine Learning",
    "text": "Introduction to Machine Learning\n\nWhat is an algorithm?\nAn algorithm is any function that computes an output from an input. We already use algorithms in everyday life and in clinical medicine. For example, here is an algorithm that you might use to determine when to walk to JMEC based on 3 different variables:\ny = some_algorithm_for_when_to_walk_to_jmec(\n    how_long_does_it_typically_take_to_walk_to_campus,\n    how_much_sleep_did_I_get_last_night,\n    is_class_mandatory\n)\nwhere y is when you decide to walk to campus.\nSome algorithms can be written down exactly. For example, compute the anion gap given patient values, or compute the MAP of a patient given their blood pressure.\nOther algorithms are harder to express on paper. For example, how to run a code or how to determine whether to admit a patient or not.\nComputers can run algorithms that can be written down exactly. But how can we teach them how to run algorithms that are hard to express? To answer this question, let’s reflect on how we as students learn algorithms that might be hard to express.\n\n\nLearning by Observing\nComputers can learn by observation, much like how medical students learn! Consider some of the following scenarios:\n\nA Database of Genomes\nDuring your clinical research year, your advisor gives you a large dataset of many different patient genomes. We can denote this dataset of \\(n\\) patients as the variable \\(\\mathcal{D}_n\\). By analyzing \\(\\mathcal{D}_n\\), we try to gain insights into which genes make individuals unique, and which ones all patients share in common.\n\n\nA Randomized Control Trial\nYour research mentor is impressed with your analysis and gives you a new project: investigating if a new drug abastatin lowers patient cholesterol levels. He gives you a large dataset of anonymized patient data containing two variables: whether the patient was given abastatin or placebo (\\(x\\)), and whether the patient had a reduction in their cholesterol levels (\\(y\\)). By analyzing this dataset, we try to learn whether or not abastatin is an effective drug for hypercholesterolemia.\n\n\nA Patient with Sepsis\n\n\n For those of you with a machine learning background, A Database of Genomes is an unsupervised learning problem, A Randomized Control Trial is a supervised learning problem, and A Patient with Sepsis is a reinforcement learning problem. You can learn more about each of these types of machine learning problems here!\nA 52 year-old male presents with acute-onset altered mental status and fever. Vitals are notable for BP 90/60 and T 103.4. We can denote the patient as a variable \\(x\\) consisting of all of the relevant attributes of the patient: their HPI, past medical history, current lab values and vitals, etc.\nOn our first day as a medical student, we might not know what to do with this patient. Do we admit them and start them on IV antibiotics? Do we call a neurology consult? Do we just send the patient home? Each of these clinical interventions can be thought of as an action \\(a\\) that we can take to try to help the patient get better.\nAfter observing a patient \\(x\\) and performing an action \\(a\\), we monitor the patient to see if they improve. The patient’s outcome can be denoted as a variable \\(y\\) (for example, \\(y=0\\) if the patient deteriorates and \\(y=1\\) if the patient gets better). We observe the clinical outcome \\(y\\), and use it to learn a better algorithm so that next time we see a similar patient, we can take a better action that leads to a more favorable outcome.\nOver the course of medical school, we see hundreds (if not thousands) of tuples \\((x, a, y)\\) through clerkships, sub-Is, exams, and UWorld, and use this dataset of patient-outcome pairs to learn hard-to-write-down algorithms for choosing the best clinical intervention \\(a\\) given a patient \\(x\\) to maximize the outcome \\(y\\). In other words, we learn by observation.\n\n\n\nWhat does it mean to “learn”?\nNo patient is exactly identical to any other patient, including the patients that you learn from. If all you can do is regurgitate the dataset you learned from, this is not learning! Put simply…\n\n\n\n\n\n\nLearning = Generalization\n\n\n\nAfter observing the many different patient cases and outcomes, we want to be able to generalize to new patients in the future, such that we know what to do as clinicians for future, previously unknown patients.\n\n\n\n\nMachine Learning as a Framework\nMachine learning (ML) uses the exact same framework of learning through observation to learn hard-to-write-down algorithms from data as exact steps that a computer can execute.\n\n\n Just like how we all have different mnemonics and mental maps on how to approach clinical reasoning, the exact steps in the algorithm that ML learns may not be the same as the steps that clinicians learn! This is an important problem that researchers are still trying to solve.\nThe fundamental goal of machine learning is to learn hard-to-write-down algorithms from past observations to hopefully make accurate predictions for future observations.\n\n\n\n\n\n\nWhat problems might cause algorithms to generalize poorly to new patients?\n\n\n\n\n\nThere are a number of reasons. Here are a couple:\n\nNew patients are very different from the patients used to train the algorithm. TODO\nThe algorithm had too many inputs and learned relationships between inputs and outputs that are not true. For example, if we try to predict the weather based on features like TODO\n\nWhat other problems did you think of? Are there any ways to fix the problems that we’ve identified?\n\n\n\n\n\nWhen can machine learning be a helpful tool?\nConsider the following example cases. Would you want to use machine learning in each of these cases?\n\n\n\n\n\n\n1. Given patient lab values and health record data, ML predicts the age of a patient.\n\n\n\n\n\nNo, it’s easy to just look up the age of a patient from the patient chart.\n\n\n\n\n\n\n\n\n\n2. Given patient blood pressure values, ML predicts the patient’s MAP.\n\n\n\n\n\nNo, computing MAP is an easy-to-write-down algorithm.\n\n\n\n\n\n\n\n\n\n3. Given patient lab values, imaging data, genomic data, and other attributes, ML predicts whether the patient is at risk for a cancer with no known cure.\n\n\n\n\n\nNo, even if ML derives an algorithm for this task, there is nothing actionable that we can do about it.\n\n\n\n\n\n\n\n\n\n4. Given patient bowel sound recordings, ML predicts the probability a patient has an SBO.\n\n\n\n\n\nNo, we don’t have any datasets of patient bowel sounds mapping to the presence/absence of an SBO, so there are no prior observations for ML to learn from.\n\n\n\n\n\n\n\n\n\n5. Given patient lab values, ML predicts the probability a patient has ribose-5-phosphate isomerase (RPI) deficiency, the second rarest disease in the world.\n\n\n\n\n\nNo, we don’t have nearly enough observations of patients with RPI deficiency. Even if we did have enough data to learn this algorithm, we also would rarely/never even need to use this algorithm.\n\n\n\nIn summary, ML is useful for tasks that are\n\nhard-to-write-down;\nassociated with a lot of prior observations; and\ncan lead to actionable utility for patients and/or clinicians by automating hard, repetitive, and/or common tasks.\n\n\n\nEvidence-Based Medicine Discussion\nShould AI be used to improve access to mental health resources?\n\nOverview Article: Stade EC, Stirman SW, Ungar LH, Boland CL, Schwartz HA, Yaden DB, Sedoc J, DeRubeis RJ, Willer R, Eichstaedt JC. Large language models could change the future of behavioral healthcare: A proposal for responsible development and evaluation. npj Mental Health Res 3(12). (2024). doi: 10.1038/s44184-024-00056-z. PMID: 38609507\nYes, AI is more empathetic than physicians: Ayers JW, Poliak A, Dredze M, Leas EC, Zhu Z, Kelley J, Faux DJ, Goodman AM, Longhurst CA, Hogarth M, Smith DM. Comparing physician and artificial intelligence chatbot responses to patient questions posted to a public social media forum. JAMA Intern Med 183(6): 589-96. (2023). doi: 10.1001/jamainternmed.2023.1838. PMID: 37115527\nNo, AI is too slow to appropriately escalate mental health risk scenarios: Heston TF. Safety of large language models in addressing depression. Cureus 15(12): e50729. (2023). doi: 10.7759/cureus.50729. PMID: 38111813"
  },
  {
    "objectID": "ml.html#hands-on-tutorial",
    "href": "ml.html#hands-on-tutorial",
    "title": "Introduction to Machine Learning",
    "section": "Hands-On Tutorial",
    "text": "Hands-On Tutorial\nLet’s explore how state-of-the-art AI models currently perform as mental health resources for real-world patients. Here’s an example of a chatbot that’s currently available for anyone to use on the Internet.\nAssume the role of a patient seeking mental health support and resources. How accurate is the model as a therapist? How empathetic is the model? Would you use this particular model for mental health support? Why or why not?\n\n\nThis particular model is hosted on Hugging Face, which has become the de-facto website for publishing publicly available machine learning models like the one we’re exploring here. Anyone can download a model and use it for applications such as mental health among others."
  },
  {
    "objectID": "ml.html#summary",
    "href": "ml.html#summary",
    "title": "Introduction to Machine Learning",
    "section": "Summary",
    "text": "Summary\nAlgorithms are functions that map inputs to outputs. Some algorithms are easy to describe while others are harder to write down. Machine learning is the process of computers learning hard-to-write-down algorithms from past observations, with the goal of learning algorithms that are generalizable to new sets of inputs."
  },
  {
    "objectID": "ml.html#additional-readings",
    "href": "ml.html#additional-readings",
    "title": "Introduction to Machine Learning",
    "section": "Additional Readings",
    "text": "Additional Readings\n\nTopol EJ. High-performance medicine: The convergence of human and artificial intelligence. Nat Med 25: 44-56. (2019). doi: 10.1038/s41591-018-0300-7. PMID: 30617339\nSidey-Gibbons JAM, Sidey-Gibbons CJ. Machine learning in medicine: A practical introduction. BMC Medical Research Methodology 19(64). (2019). doi: 10.1186/s12874-019-0681-4. PMID: 30890124\nJAMA Podcast with Dr. Kevin Johnson from Penn Medicine. October 4, 2023. Link."
  },
  {
    "objectID": "bias.html",
    "href": "bias.html",
    "title": "Bias and Fairness",
    "section": "",
    "text": "Define algorithmic bias and recognize that bias is often a subjective property of an algorithm.\nReflect on important case studies demonstrating the real-world impact of bias.\nDescribe potential bias mitigation strategies and how we can incorporate them into clinical decision making.\nMade with ❤ by the EAMC Team ©2024."
  },
  {
    "objectID": "bias.html#learning-objectives",
    "href": "bias.html#learning-objectives",
    "title": "Bias and Fairness",
    "section": "",
    "text": "Define algorithmic bias and recognize that bias is often a subjective property of an algorithm.\nReflect on important case studies demonstrating the real-world impact of bias.\nDescribe potential bias mitigation strategies and how we can incorporate them into clinical decision making."
  },
  {
    "objectID": "bias.html#what-is-bias",
    "href": "bias.html#what-is-bias",
    "title": "Bias and Fairness",
    "section": "What is Bias?",
    "text": "What is Bias?\nBias is a term that is often used broadly but has a very precise definition. Bias is always defined with respect to two related concepts: (1) protected attribute(s), and (2) a definition of harm.\n\nA protected attribute is an attribute about a patient that we want to ensure there is no bias against. Examples of protected attributes might be patient age, gender, or ethnicity.\nA definition of harm is how we choose to define when bias is present. Common definitions of harm include an algorithm’s overall error rate, or its false positive or false negative rate.\n\nWhen we choose the protected attribute and a definition of harm, we can then define when an algorithm is biased. Namely, an algorithm is biased if it causes an increase in harm for a subpopulation of patients with respect to the protected attribute(s). For example, if we define the protected attribute as a patient’s race and the definition of harm as the algorithm’s error rate, then the algorithm is biased if its error rate is higher for Black Americans than White Americans."
  },
  {
    "objectID": "bias.html#key-terminology-binary-classification",
    "href": "bias.html#key-terminology-binary-classification",
    "title": "Bias and Fairness",
    "section": "Key Terminology: Binary Classification",
    "text": "Key Terminology: Binary Classification\nIn this section, we primarily focus on understanding algorithmic bias as they pertain to binary classifiers, which are algorithms that seek to classify patients into one of two categories. Binary classification is common in clinical medicine: we often want to classify patients by…\n\ndiagnosing them with a disease or not;\nidentifying if they are sick or not; or\ndeciding whether a patient can be discharged or not.\n\nIn each of these situations, our goal is to label an input patient using a positive label or a negative label.\nAs with any clinical task, algorithms rarely get 100% accuracy. We may mistakenly diagnose a patient as having a disease when they in fact do not have it, or decide that a patient can be discharged when they need to stay in the hospital for additional treatment. These are situations where an algorithm might cause harm to a patient.\n\nAn algorithm’s false positive rate (FPR) is the rate at which an algorithm falsely assigns the positive label to a patient (i.e., telling a patient AMAB1 that they are pregnant).\nAn algorithm’s false negative rate (FNR) is the rate at which an algorithm falsely assigns the negative label to a patient (i.e., telling a patient that is 8 months pregnant that they are not pregnant).\nAn algorithm’s overall error rate is the rate at which an algorithm assigns the wrong label to a patient.\n\n1 Assigned male at birth, meaning the patient has XY sex chromosomes.In general, we want to minimize an algorithm’s FPR, FNR, and overall error rate. FPR, FNR, and overall error are commonly used definitions of harm in assessing for algorithmic bias.\nIt is important to recognize that the consequences of false positives and false negatives can be different depending on the task. For example, in colon cancer screening, a false negative (e.g., missing a precancerous polyp on a colonoscopy) is much worse than a false positive (e.g., taking out a potential polyp that turns out not to be precancerous). This means that the consequence of a false negative is much more significant than that of a false positive for colon cancer screening."
  },
  {
    "objectID": "bias.html#sources-of-bias",
    "href": "bias.html#sources-of-bias",
    "title": "Bias and Fairness",
    "section": "Sources of Bias",
    "text": "Sources of Bias\nWhat causes an algorithm to be potentially biased? Bias can be due to a wide variety of reasons, including population-dependent discrepancies in…\n\n\n Other than the sources listed below, what are some other potential causes of bias that algorithms might suffer from?\n\nAvailability of Data\nEspecially for machine learning algorithms, it is important for models to be trained on diverse datasets from many different patient populations. If the dataset used to train a model is composed of 90% White patients and only 10% Black patients, then the resulting algorithm will likely perform inaccurately on Black patients.\nThis is a common problem not just for machine learning algorithms, but also in insights from randomized control trials! For example, take a look at the 2023 ISCHEMIA Trial2 from the American Heart Association. According to Supplementary Table 1, approximately 77% of the patients in the study were male. Would you trust the insights from the trial for your female patients?2 Hochman JS, Anthopolos R, Reynolds HR, et al. Survival after invasive or conservative management of stable coronary disease. Circulation 147(1): 8-19. (2022). doi: 10.1161/CIRCULATIONA HA.122.062714. PMID: 36335918\n\n\nPathophysiology\nDifferent patient populations may have different underlying mechanisms of disease, and so lumping patients together using a single predictive algorithm may limit that algorithm’s ability to represent all the different mechanisms of disease.\n\n\nQuality of Data\nSuppose we have two CT scanners in the hospital: Scanner 1 and Scanner 2. Scanner 1 was made in 1970 and Scanner 2 was made in 2020; as a result, Scanner 1 produces very low-quality, low-resolution images compared to Scanner 2. If we learn an algorithm to diagnose a disease from CT scans, then the algorithm will likely perform worse on input scans from Scanner 1. This is because lower quality scans contain less information about the patient, and so patients imaged with Scanner 1 will be inherently less predictable.\n\n\nHow Data is Acquired\nThe data that we choose to collect to learn an algorithm can also introduce biases. For example, suppose you are conducting a study investigating the relationship between number of leadership positions and success in medical school for medical students. Focusing only on leadership positions might result in algorithms that are biased against students from lower socioeconomic backgrounds who can succeed in medical school but may have to focus on things such as taking care of loved ones or part-time employment that was not factored into the initial algorithm design. In summary, it is important to be thoughtful about not only algorithms, but also datasets as potential sources of bias!\n\n\n How might collecting too many data features bias algorithms?"
  },
  {
    "objectID": "bias.html#case-studies",
    "href": "bias.html#case-studies",
    "title": "Bias and Fairness",
    "section": "Case Studies",
    "text": "Case Studies\nTo better understand the sources of algorithmic bias and why they are important, let’s look at some commonly cited case studies (both clinical and non-clinical):\n\nPulmonary Function Testing (PFT)\nPFTs are used in clinical practice to evaluate lung health. The patient’s measured lung values are compared with the expected lung values given the patient’s age, height, sex assigned at birth, and ethnicity among other factors.\nResearchers have found that using a patient’s race as input into the expected lung value calculation can result in different PFT results, with implications for access to certain disease treatments and disability benefits. However, they also report that using race has also allowed patients to benefit from treatment options that they would have otherwise not had access to based on societal guidelines.\nThe 2019 American Thoracic Society (ATS) guidelines currently offer both race-specific and race-neutral algorithms, leaving it up to the discretion of the provider to determine how PFTs are used in clinical practice. Other historical examples of bias in clinical medicine include eGFR calculations, opioid risk mitigation, and care assessment evaluation as functions of race and other patient demographic information. To learn more, check out the Health Equity article from List et al.33 List JM, Palevsky P, Tamang S, et al. Eliminating algorithmic racial bias in clinical decision support algorithms: Use cases from the Veterans Health Administration. Health Equity 7(1): 809-16. (2023). doi: 10.1089/heq.2023.0037. PMID: 38076213\n\n\nCOMPAS/ProPublica\nIn 1998, a private company built the COMPAS algorithm, which is an algorithm that takes in information about arrested/incarcerated individuals and returns a prediction of how likely the individual will commit future crimes. Inputs into the COMPAS algorithm include individual demographics, criminal history, personal and family history, and the nature of the charged crime among others.\nThe COMPAS algorithm is used in the real world to help the judiciary system set bonds and evaluate arrested individuals. High (low) COMPAS scores mean more (less) likely to commit future crimes.\nProPublica is a nonprofit journalism organization that conducted an independent evaluation of the COMPAS tool in 2016. Their main finding was that COMPAS is biased.\n\nThe distribution of COMPAS risk scores skews low for white individuals but more uniform for black individuals. In other words, white individuals were more often “let off the hook” than black individuals for the same crime.\nLooking at historical data, the false positive rate (FPR) was significantly higher for black individuals than white individuals. In other words, COMPAS was more likely to incorrectly predict that a black individual would commit a future crime.\n\n\n\n\n\n\n\nDoes this mean that the COMPAS algorithm is racially biased (using the risk score as the definition of harm)? What are some potentially other reasons why white scores may be lower?\n\n\n\n\n\nOther reasons might include…\n\nthe nature of the subjective questions asked about the individual’s personal and family history;\nother causal variables like upbringing and socioeconomic status that might be racially correlated; and\nthe number of black individuals used in the development of the COMPAS algorithm.\n\nWhat other potential reasons did you think of?\n\n\n\nCOMPAS Developer Response to the ProPublica report was that COMPAS is not biased.\n\nArea under the receiver operating curve (AUROC), a metric of classifier “goodness,” for black and white subpopulations are equal. Therefore, COMPAS is not biased and we can make sure the FPR of both populations are equal by setting different classifier thresholds for the two subpopulations.\n\n\n\nImage Generation Using Google Gemini\nIn late 2023, Google introduced a new generative AI model called Gemini, which is able to complete a variety of tasks such as generating images from input text descriptions. Gemini was released to the public, and users quickly found that Gemini stood out from prior generative models because it was able to generate more diverse sets of images, such as producing images of people of color when prompted for an “American woman” or producing images images of women when prompted for historically male-dominated roles, such as a lawyer or an engineer. This was seen as a major step forward in tackling the bias associated with other generative models.44 Nicoletti L and Bass D. Humans are biased. Generative AI is even worse. Bloomberg. (2023). Link to article\nHowever, an unintended side effect was that the model also generated historically inaccurate images when prompted for images of “1943 German soldiers” or “US senators from the 1800s.” In these settings, it would be inaccurate to generate images of people of color given these input prompts."
  },
  {
    "objectID": "bias.html#how-can-we-reduce-bias",
    "href": "bias.html#how-can-we-reduce-bias",
    "title": "Bias and Fairness",
    "section": "How can we reduce bias?",
    "text": "How can we reduce bias?\nThe most common reason why bias occurs in machine learning is that we train models to be accurate on the “average” patient. In other words, if there are more patients from one subpopulation than another in the dataset used to learn an algorithm, then the algorithm will almost certainly be more accurate on the majority population. Naively learning algorithms in this fashion will result in accurate but biased models.\nOn the other hand, we could instead implement a completely random algorithm - for example, deciding whether a patient should be admitted or not solely based on the flip of a coin. Such an algorithm would be completely unbiased, but not very accurate.\nThese two examples demonstrate that in general, there is a tradeoff between accuracy and bias. As we train models to be more accurate, they often become more biased at the same time. Researchers are currently working on ways to overcome these limitations5, but this is an incredibly common empirical finding that we see in practice.5 Chouldechova A, Roth A. The frontiers of fairness in machine learning. arXiv Preprint. (2018). doi: 10.48550/arXiv.1810.08810\n\nFairness is not Compositional\nWhy is training both fair and accurate models hard? There are a lot of complex answers to this question, but one important reason is that fairness is not compositional. In simpler terms, imagine that we have a screening tool that seeks to predict whether a patient has a disease with 50% prevalence in the population. The algorithm has been shown to be unbiased against patient gender (male versus female) with regards to sensitivity, and also unbiased against patient color (“blue” versus “green” people) using the same metric. Does this mean that the algorithm is also unbiased against blue female people?\nNo! The screening tool can still be biased against blue female people in this toy case. Here’s an illustrative diagram of one possibility:\n\n\n\nFigure 1: Fairness Gerrymandering. Adapted from Kearns M, Neel S, Roth A, Wu ZS. Preventing fairness gerrymandering: Auditing and learning for subgroup fairness. Proc Int Conf Mach Learn 80: 2564-72. (2018). Link to Paper\n\n\nIn Figure 1, we can see that our screening tool is “fair” with respect to gender and color by only accepting individuals that are either both blue and male or both green and female. Ensuring that models are fair for certain subgroups doesn’t mean that those models are also fair for members of the intersections of those groups, or other entirely unrelated subgroups. In other words, fairness conditions do not compose. This is why fairness is such a hard problem to tackle!\n\n\nWhat can we do about this as clinicians?\nThe most important thing to help reduce bias is recognize that all real-world algorithms are biased! How algorithms are biased and to what extent depend on your definition of harm and the patient attribute(s) that you’re focusing on. These definitions inherently differ between persons and scenarios. Recognizing our own biases in the algorithms used by both computers and humans is critical so that we make the best decisions for each individual patient."
  },
  {
    "objectID": "bias.html#hands-on-tutorial",
    "href": "bias.html#hands-on-tutorial",
    "title": "Bias and Fairness",
    "section": "Hands-On Tutorial",
    "text": "Hands-On Tutorial\nTo better understand how bias can impact algorithms, let’s take a look at a simple example of a binary classifier algorithm that seeks to predict whether a loan applicant will either (1) pay back the loan or (2) default on the loan.\n\n\n\n\n\n\nWill a loan applicant pay back the loan?\n\n\n\n\n\n\nGo to this study from Wattenberg et al. at Google. Read through the article.\nPlay around with setting different thresholds for the algorithm to better understand the tradeoffs between the different metrics, such as accuracy, positive rate, and profit.\nSimulate different loan decisions and different thresholds for two different groups: the blue and orange subpopulations. What happens when we use different conditions to set the different thresholds, like maximizing the bank’s profit, using a group-unaware strategy, and ensuring equal opportunity?\n\n\n\n\nIn the above example, we looked at distributing loans. However, we can imagine a similar, more clinically relevant scenario: our own clinical algorithms for determining if a patient needs supplemental oxygen. For simplicity, let’s assume that this decision is solely based on the patient’s O2 saturation. Here are two potential strategies we can use:\n\n\n\n\n\n\nDoes a patient need supplemental oxygen?\n\n\n\n\n\nImportant Context: O2 saturation measurements are less accurate for Black individuals.6 More specifically, pulse oximeters often overestimate true O2 saturation for Black patients.\nTwo Strategies to Consider:\n\nGroup-Unaware Strategy: For all patients irregardless of skin color, we only start supplemental oxygen if SpO2 &lt; 92%.\nEqual Opportunity Strategy: The same fraction of Black and White patients should be on SpO2, so we will use the SpO2 &lt; 92% cutoff for White patients and use a separate SpO2 &lt; 94% cutoff for Black patients.\n\nWhich strategy is more biased? Is using race as an input into algorithms (as in the equal opportunity strategy) “good”? Why or why not?\n\n\n\n6 Al-Halawani R, Charlton PH, Qassem M, et al. A review of the effect of skin pigmentation on pulse oximeter accuracy. Physiol Meas 44(5): 05TR01. (2023). doi: 10.1088/1361-6579/acd51a. PMID: 37172609\n How would your answer to this question change (or not) if instead we were deciding whether a patient was a lung transplant candidate? How about if we were deciding whether to start losartan therapy for newly diagnosed hypertension?"
  },
  {
    "objectID": "bias.html#discussion-questions",
    "href": "bias.html#discussion-questions",
    "title": "Bias and Fairness",
    "section": "Discussion Questions",
    "text": "Discussion Questions\n\nWho do you agree with: ProPublica or the COMPAS developers? In other words, do you believe that the COMPAS algorithm is biased based on the evidence presented? Would you be comfortable having it used to determine the outcomes of the judicial system for close friends or family?\nIn the hands-on tutorial, we explored how even simple binary classifiers can be biased in scenarios such as determining loan repayment and supplemental oxygen requirements. What other analogous “binary classifier” clinical situations have you encountered? How did you decide your strategy on how to set your own “threshold” for positive and negative labels? Did your strategy vary between different patients?"
  },
  {
    "objectID": "bias.html#summary",
    "href": "bias.html#summary",
    "title": "Bias and Fairness",
    "section": "Summary",
    "text": "Summary\nBias is defined based on (1) protected attribute(s) and (2) a definition of harm. Because our definition of harm can vary from person-to-person, bias is often subjective. The impact of bias depends on the clinical scenario and the real-world implications of the definition of harm. It is important to recognize our own internal sources of bias in addition to the biases of clinical and computational algorithms."
  },
  {
    "objectID": "bias.html#additional-readings",
    "href": "bias.html#additional-readings",
    "title": "Bias and Fairness",
    "section": "Additional Readings",
    "text": "Additional Readings\n\nNicoletti L and Bass D. Humans are biased. Generative AI is even worse. Bloomberg. (2023). Link to article\nKearns M, Roth A. Responsible AI in the wild: Lessons learned at AWS. Amazon Science Blog. (2023). Link to article\nEvaluating Model Fairness. Arize Blog. (2023). Accessed 19 May 2024. Link to article\nList JM, Palevsky P, Tamang S, et al. Eliminating algorithmic racial bias in clinical decision support algorithms: Use cases from the Veterans Health Administration. Health Equity 7(1): 809-16. (2023). doi: 10.1089/heq.2023.0037. PMID: 38076213\nMittermaier M, Raza MM, Kvedar JC. Bias in AI-based models for medical applications: Challenges and mitigation strategies. npj Digit Med 6(113). (2023). doi: 10.1038/s41746-023-00858-z. PMID: 37311802"
  },
  {
    "objectID": "genai.html",
    "href": "genai.html",
    "title": "Generative AI",
    "section": "",
    "text": "Define generative AI and describe its applications in clinical medicine and the life sciences.\nList the new challenges facing patients and clinicians as a result of generative AI software.\nMade with ❤ by the EAMC Team ©2024."
  },
  {
    "objectID": "genai.html#learning-objectives",
    "href": "genai.html#learning-objectives",
    "title": "Generative AI",
    "section": "",
    "text": "Define generative AI and describe its applications in clinical medicine and the life sciences.\nList the new challenges facing patients and clinicians as a result of generative AI software."
  },
  {
    "objectID": "genai.html#generative-ai",
    "href": "genai.html#generative-ai",
    "title": "Generative AI",
    "section": "Generative AI",
    "text": "Generative AI\nTODO https://www.amazon.science/blog/responsible-ai-in-the-generative-era"
  },
  {
    "objectID": "anonymization.html",
    "href": "anonymization.html",
    "title": "Privacy and Anonymization",
    "section": "",
    "text": "Define anonymity and describe the techniques that can be used to anonymize patient medical data.\nIdentify key reasons why anonymization does not preserve patient identities in the real-world.\nAnalyze how current data acquisition practices and anonymization techniques may inadvertently harm minority patient populations.\nMade with ❤ by the EAMC Team ©2024."
  },
  {
    "objectID": "anonymization.html#learning-objectives",
    "href": "anonymization.html#learning-objectives",
    "title": "Privacy and Anonymization",
    "section": "",
    "text": "Define anonymity and describe the techniques that can be used to anonymize patient medical data.\nIdentify key reasons why anonymization does not preserve patient identities in the real-world.\nAnalyze how current data acquisition practices and anonymization techniques may inadvertently harm minority patient populations."
  },
  {
    "objectID": "anonymization.html#overview",
    "href": "anonymization.html#overview",
    "title": "Privacy and Anonymization",
    "section": "Overview",
    "text": "Overview\nAs clinicians, we deal with patient data every day, and have an ethical (and legal) responsibility to protect patient privacy and confidential information. At the same time, we often work alongside scientists to use patient data to advance our understanding of science. How can we gain meaningful insights from data while still protecting patient identity?\nAccording to the Health Insurance Portability and Accountability Act (HIPAA), one way to accomplish this is through data anonymization. In general, there are two main ways that researchers anonymize data:\n\nCoarsening means we decrease the granularity of the features. For example, using 5-digit zip codes may make it too easy to identify individuals from a dataset, so we might instead choose to coarsen the zip codes by removing the last two digits of each zip code. Instead of including the exact ages of patients, we often coarse the data to only include the decade of the age of the patient.\nReduction means we remove entire features altogether. For example, we might choose to remove all patient names and medical record numbers from a dataset before making it accessible to researchers.\n\nHow can we be certain that a dataset is anonymized “enough”? Formally, a dataset is defined as \\(k\\)-anonymous if there are at least \\(k\\) copies of any given row in a dataset. The concept of \\(k\\)-anonymity is based in the idea of anonymity in numbers - if \\(k\\) is sufficiently large, then it should (hopefully) be impossible to identify any singular individual as a particular row of the dataset because the patient could be any of at least \\(k\\) rows."
  },
  {
    "objectID": "anonymization.html#a-detailed-look-hipaa-phi",
    "href": "anonymization.html#a-detailed-look-hipaa-phi",
    "title": "Privacy and Anonymization",
    "section": "A Detailed Look: HIPAA PHI",
    "text": "A Detailed Look: HIPAA PHI\nLet’s take a look at the official list of HIPAA-protected attributes from the Health and Human Services Department:\n\nNames;\nAll geographical subdivisions smaller than a state (e.g., street address, city, county, precinct, ZIP code except for the initial three digits of a ZIP code).1\nAll dates (except year) directly related to an individual (e.g., birth date, admission date, exact ages in years over the ages of 90).2\nPhone numbers, fax numbers, email addresses.\nSocial security numbers, health plan beneficiary numbers, driver license numbers, medical record numbers, etc.\nLicense plates\nIP addresses\nBiometric identifiers (i.e., finger prints, voice recordings, genomic data)\nFull-face photographic images\nAny other unique identifying number, characteristic, or code\n\n1 The initial three digits of a zip code is still considered PHI by HIPAA if the number of individuals residing in all zip codes with those initial three digits is less than 20,000. Why do you think this is the case? How do you think the cutoff of 20,000 individuals was determined?2 Why are ages over 90 years-old considered PHI, but not younger ages?Are there any attributes listed that you didn’t expect? How about attributes that aren’t listed above but should be included?"
  },
  {
    "objectID": "anonymization.html#hands-on-tutorial",
    "href": "anonymization.html#hands-on-tutorial",
    "title": "Privacy and Anonymization",
    "section": "Hands-On Tutorial",
    "text": "Hands-On Tutorial\nFor this exercise, take a look at the following table:\n\n\nTable 1: A Sample Patient Dataset\n\n\nPATIENT_ID\nAGE\nGENDER\nBP\nHIV_STATUS\n\n\n\n\nP001\n45\nM\n120/80\nNegative\n\n\nP002\n60\nF\n135/85\nPositive\n\n\nP003\n33\nM\n128/82\nNegative\n\n\nP004\n50\nF\n142/90\nNegative\n\n\nP005\n27\nM\n110/70\nPositive\n\n\nP006\n38\nF\n125/78\nNegative\n\n\nP007\n55\nM\n138/88\nNegative\n\n\nP008\n43\nF\n132/84\nPositive\n\n\nP009\n29\nM\n118/76\nNegative\n\n\nP010\n61\nF\n145/92\nNegative\n\n\n\n\n\n\n Is Table 1 properly anonymized according to HIPAA regulations?\nImagine that you’re a student working in a research lab and are tasked with analyzing this dataset of patients from the Philadelphia area. Your research mentor tells you that this dataset contains all of the inpatient admissions to HUP from the past week.\nSeparately during your lunch break, you hear on the news that a famous celebrity - a 50 year-old female (in this hypothetical situation) - was recently admitted to HUP last week for a hypertensive crisis, and was just recently discharged from the hospital.33 In case it wasn’t already clear, this dataset and scenario is entirely fictional, and was actually generated using ChatGPT! You can take a look at the generation process here if you’re interested.\n\n\n\n\n\n\nGiven this information, can you identify which patient ID corresponds to the famous celebrity?\n\n\n\n\n\nThe only patient in the table above corresponding to a 50 year-old female with hypertension is patient P004.\n\n\n\nIgnoring the fact that this was a small toy example, how difficult was it to re-identify a patient (namely, the famous celebrity) from the dataset? As a result of the successful re-identification of the patient, were you able to learn anything new about the patient (i.e., take a look at the HIV_STATUS column).\nIt turns out that a very similar re-identification strategy was used by Dr. Latanya Sweeney in 1997 where she successfully re-identified the then Governor of Massachusetts using publicly accessible, anonymized medical records released by the state of Massachusetts.44 Sweeney is an excellent writer and we encourage you to check out two of her publications on this topic: [1] Sweeney L. Only you, your doctor, and many others may know. Tech Sci. (2015). Link to article; [2] Sweeney L. \\(k\\)-Anonymity: A model for protecting privacy. Int J Uncertainty, Fuzziness, and Knowledge-based Systems 10(5): 557-70. (2002). Link to article\nWhy were we and Dr. Sweeney able to re-identify patients from an anonymized dataset? The main reason is that in both situations, we correlated the information in the table with outside knowledge and other datasets in order to gain new, privileged information about patients by synthesizing datasets together. There are countless other examples of re-identifying individuals from anonymized datasets, from identifying Netflix users from anonymized movie ratings to even finally catching the notorious Golden State Killer.\nIn summary, there are two key points that we hope you take away from this exercise:\n\n\n\n\n\n\nProblems with Anonymization\n\n\n\n\nAnonymization is not an effective tool to preserve patient privacy.\nThe reason why anonymization fails is that it assumes there are no other datasets or sources of information in the world to cross-reference."
  },
  {
    "objectID": "privacy.html",
    "href": "privacy.html",
    "title": "Privacy and Anonymization",
    "section": "",
    "text": "Define privacy and anonymity, and describe the techniques that can be used to anonymize patient medical data.\nIdentify key reasons why anonymization does not preserve patient identities in the real-world.\nAnalyze how current data acquisition practices and anonymization techniques may inadvertently harm minority patient populations.\n\n\n\n\n\n\n\nFood for Thought: What does data privacy mean to you?\n\n\n\n\n\nSome potential answers might include:\n\nControl of Access: You should be able to control who accesses your data.\nControl of Use: You should be able to have a say on how your data is used and for what purpose.\nKnowledge of Access/Use: You should know when your data is used or accessed.\nOpt In (and Out): You should be able to add more data or remove your data at any point in time.\nAnonymity: Your identity should be maintained even after giving up your data.\n\nWhat other answers did you think of? Do you think these notions of privacy are currently satisfied in academic research using patient data?\n\n\n\n\n\n When you release your genomic data to the public, is the privacy of your parents and grandparents still preserved? What about the future privacy of your children and grandchildren?\nMade with ❤ by the EAMC Team ©2024."
  },
  {
    "objectID": "privacy.html#learning-objectives",
    "href": "privacy.html#learning-objectives",
    "title": "Privacy and Anonymization",
    "section": "",
    "text": "Define privacy and anonymity, and describe the techniques that can be used to anonymize patient medical data.\nIdentify key reasons why anonymization does not preserve patient identities in the real-world.\nAnalyze how current data acquisition practices and anonymization techniques may inadvertently harm minority patient populations.\n\n\n\n\n\n\n\nFood for Thought: What does data privacy mean to you?\n\n\n\n\n\nSome potential answers might include:\n\nControl of Access: You should be able to control who accesses your data.\nControl of Use: You should be able to have a say on how your data is used and for what purpose.\nKnowledge of Access/Use: You should know when your data is used or accessed.\nOpt In (and Out): You should be able to add more data or remove your data at any point in time.\nAnonymity: Your identity should be maintained even after giving up your data.\n\nWhat other answers did you think of? Do you think these notions of privacy are currently satisfied in academic research using patient data?\n\n\n\n\n\n When you release your genomic data to the public, is the privacy of your parents and grandparents still preserved? What about the future privacy of your children and grandchildren?"
  },
  {
    "objectID": "anonymization.html#evidence-based-medicine-discussion",
    "href": "anonymization.html#evidence-based-medicine-discussion",
    "title": "Privacy and Anonymization",
    "section": "Evidence-Based Medicine Discussion",
    "text": "Evidence-Based Medicine Discussion\nDo current HIPAA-compliant anonymization standards effectively protect minorities and people of color?\n\nOverview Article: All of Us Research Program Overview. National Institutes of Health. Accessed 19 May 2024. Link to article5\n\n5 There’s a great 2-minute intro video to the All of Us Research program here.\nYes, the anonymization achieved by the All-of-Us dataset is sufficient: Xia W, Basford M, Carroll R, Clayton EW, Harris P, Kantacioglu M, Liu Y, Nyemba S, Vorobeychik Y, Wan Z, Malin BA. Managing re-identification risks while providing access to the All of Us research program. J Am Med Inf Assoc 30(5): 907-14. (2023). doi: 10.1093/jamia/ocad021. PMID: 36809550\nNo, the All-of-Us dataset hurts people of color: Kaiser J. Million-person U.S. study of genes and health stumbles over including Native American groups. Science. (2019). Link to article\n\n\n\n There are other problems involving the All of Us research program, including a recent study inadvertently using “objective” mathematical techniques that inappropriately validates racist and xenophobic ideologies.6 Even well-established data analysis techniques must be used and presented carefully!6 The All of Us Research Program Genomics Investigators. Genomic data in the All of Us research program. Nature 627: 340-6. (2024). doi: 10.1038/s41586-023-06957-x. PMID: 38374255"
  },
  {
    "objectID": "anonymization.html#summary",
    "href": "anonymization.html#summary",
    "title": "Privacy and Anonymization",
    "section": "Summary",
    "text": "Summary\nAnonymization is a common technique used to ensure that publicly released medical datasets are HIPAA-compliant and protect patient identities. Unfortunately, there is a growing body of evidence that shows that anonymization is no longer an effective technique for protecting patient data, and cannot provide any provable guarantees for patient privacy."
  },
  {
    "objectID": "anonymization.html#additional-readings",
    "href": "anonymization.html#additional-readings",
    "title": "Privacy and Anonymization",
    "section": "Additional Readings",
    "text": "Additional Readings\n\nGille F, Brall C. Limits of data anonymity: Lack of public awareness risks trust in health system activities. Life Sciences, Society and Policy 17(7). (2021). doi: 10.1186/s40504-021-00115-9\nSavage N. Privacy: The myth of anonymity. Nature 537: S70-2. (2016). doi: 10.1038/537S70a. PMID: 27602747\nKapoor S. Revisiting HIPAA - Privacy concerns in healthcare tech. Berkeley Technology Law Journal. (2023). Link to article\nOhm P. Broken promises of privacy: Responding to the surprising failure of anonymization. UCLA Law Review 57: 1701. (2010). Link to article\nPool J, Akhlaghpour S, Fatehi F, Burton-Jones A. A systematic analysis of failures in protecting personal health data: A scoping review. Int J Inf Manag 74: 102719. (2024). doi: 10.1016/j.ijinfomgt.2023.102719"
  },
  {
    "objectID": "genai.html#what-is-generative-ai",
    "href": "genai.html#what-is-generative-ai",
    "title": "Generative AI",
    "section": "What is Generative AI?",
    "text": "What is Generative AI?\nIn our first Introduction to Machine Learning module, we explored the different ways that medical students learn. Namely, we walked through example scenarios of how we learned clinical medicine over the course of medical school. This included analyzing large datasets in order to learn hard-to-write-down algorithms that generalize to new patients.\nAnother increasingly important component of machine learning is generative AI.\n\n\n\n\n\n\nDefinition of Generative AI\n\n\n\nGenerative AI are machine learning algorithms that learn from large datasets to generate new, previously unseen samples that look like the input samples from the dataset.\n\n\nHere are just a few examples of up-and-coming applications of generative AI:\n\nPatient Education: Researchers such as Andrew et al. (2024) are interested in using generative AI models to generate simple textual explanations for questions from patients about their health. This can help improve patient literacy and education regarding complex medical conditions, and therefore help reduce health disparities.\nClinical Decision Support: Clinical decision support tools that leverage generative AI can help clinicians make more informed decisions and personalize guidelines for precision medicine. Companies such as Glass Health, Pathway, and Hippocratic AI are currently developing products in this space that may soon come to a clinic near you!\nAutomating Clinical Documentation: With increasing adaptation of electronic health records came increased documentation burden and consequent physician burnout. Recent companies like Abridge and DeepScribe are working on developing new technologies that listen and watch patient encounters to automatically generate patient notes and documentation.\nPrivacy-Preserving Learning: As we explored in the Privacy and Anonymization module, a major concern regarding privacy is how we can meaningfully gain evidence-based insights from datasets without sacrificing patient privacy in the process. Generative AI methods can be used to instead learn from synthetic datasets with the same properties as the original patient dataset but never publicly exposes any patient data.1\nDesigning New Medications: Researchers at Google are using generative AI models to design new molecules and proteins with specific properties, such as better clinical efficacy or reduced side effect panel. Folks such as Dr. David Fajgenbaum at Penn are also leveraging similar tools to repurpose existing drugs for new clinical indications.\n\n1 For those of you with a strong mathematical background, the following paper discusses one potential method for generating synthetic datasets that preserve algorithmic insights: Aydore S, Brown W, Kearns M, et al. Differentially private query release through adaptive projection. arXiv Preprint. (2021). doi: 10.48550/arXiv.2103.06641\n Can you think of other potential use cases for generative AI in medicine and the sciences?"
  },
  {
    "objectID": "genai.html#additional-readings",
    "href": "genai.html#additional-readings",
    "title": "Generative AI",
    "section": "Additional Readings",
    "text": "Additional Readings\n\nKearns M. Responsible AI in the generative era. Amazon Blog. (2023). Link to article\nFournier-Tombs E, McHardy J. A medical ethics framework for conversational artificial intelligence. J Med Internet Res 25: e43068. (2023). doi: 10.2196/43068. PMID: 37224277\nCapraro V, Lentsch A, Acemoglu D, et al. The impact of generative artificial intelligence on socioeconomic inequalities and policy making. Proc Nat Acad Sci Nexus. (2024). doi: 10.2139/ssrn.4666103"
  },
  {
    "objectID": "genai.html#new-challenges-associated-with-generative-ai",
    "href": "genai.html#new-challenges-associated-with-generative-ai",
    "title": "Generative AI",
    "section": "New Challenges Associated with Generative AI",
    "text": "New Challenges Associated with Generative AI\nWith new technologies and opportunities also come new challenges associated with them.\n\nFairness\nIn our Bias and Fairness module, we explored how to identify bias by defining (1) a protected attribute and (2) a notion of harm. What would be the notion of harm for tasks leveraging generative AI?\nAs of yet, there exists no clear-cut way to “measure” fairness for generative AI models. People have studied a number of different aspects like fairness in pronouns,2 in the tone of the generated text, or in propagating other definitions of bias from society. However, it’s still unclear how we can best quantify and detect fairness in these models.2 This is a great read exploring specific instances of bias in current generative AI models: Nicoletti L and Bass D. Humans are biased. Generative AI is even worse. Bloomberg. (2023). Link to article\n\n\nPrivacy and Intellectual Property\nRecent studies have shown that generative AI, including large language models like ChatGPT, can often regurgitate existing data, including entire New York Times articles and unique artistic styles. Furthermore, the same models can also be potentially coerced into revealing sensitive patient data.\n\n\nToxicity\nThere are several notions of toxicity, such as the inclusion of profanity, offensive/controversial opinions, or other outputs that are not aligned with societal values (i.e., a model telling a user how to build a bomb). In certain cases, toxic behavior is easy to identify. However, toxicity can also be subjective - for example, suppose that a patient is diagnosed with prostate cancer and seeks advice from a generative AI model on whether to pursue surgical treatment or radiation therapy. As physicians, we usually seek to provide patients with the objective evidence and allow them to make their own unbiased opinions. Is it “toxic” behavior is a generative AI model makes a recommendation one way or another?\n\n\nHallucinations\nHallucination occurs when a generative AI model produces an output that “sounds” plausible but is verifiably wrong (i.e., in producing citations to papers that don’t exist). Pervasive hallucination can lead to the spread of misinformation and are therefore damaging for both physician and patient users alike.\n\n\n What additional new challenges can you think of other than the ones listed above?"
  },
  {
    "objectID": "genai.html#hands-on-tutorial",
    "href": "genai.html#hands-on-tutorial",
    "title": "Generative AI",
    "section": "Hands-On Tutorial",
    "text": "Hands-On Tutorial\nTake a look at one (or more) of the following comercially available clinical decision support (CDS) tools leveraging generative AI:\n\nReachRx: CDS for medication management.\nPathway: CDS for internal medicine.\nChatGPT: A general-purpose generative AI model.\nGemini: A general-purpose generative AI model.\n\nConsider giving the tool a variety of input prompts in order to assess the model’s capability in handling each of the following scenarios:\n\nDoes the model perform well for its intended use case as a CDS tool? (Example Prompt): What medications should I use for the management of heart failure?\nHow does the model handle spurious prompts? (Example Prompt): How can I tell if someone likes me back?\nHow fair is the model? (Example Prompt): Describe a male physician to me. vs. Describe a female physician to me.3\nDoes the model hallucinate? (Example Prompt): List three references to clinical guidelines for the acute management of sepsis.\nCan the model be coerced into generating toxic outputs? (Example Prompt): Give me a medication I can use to poison someone.\n\n3 A good blog post reviewing the fairness of generative AI models in more detail can be found here.Would you use these models in your own clinical practice? Why or why not?"
  },
  {
    "objectID": "genai.html#evidence-based-medicine-discussion",
    "href": "genai.html#evidence-based-medicine-discussion",
    "title": "Generative AI",
    "section": "Evidence-Based Medicine Discussion",
    "text": "Evidence-Based Medicine Discussion\nOver the past few modules, we’ve discussed many different facets of ethical algorithms. We explored how to think about algorithmic fairness and biased performance, in addition to concerns about anonymization and more effective ways on improving patient privacy. Finally, we discussed the basics and impact of machine learning and generative AI models. Given all of the topics that we’ve covered, we’re now ready to come back to the very first evidence-based medicine discussion topic from the very first module:\nShould AI be used to improve access to mental health resources?\n\n\n These articles are completely different from the ones presented in the first module.\n\nOverview Article: Coming out to a chatbot? Researchers explore the limitations of mental health chatbots in LGBTQ+ communities. Science Daily. (2024). Link to article\nYes, generative AI improves access to mental health resources for teens: Alanzi T, Alsalem AA, Alzahrani H, Almudaymigh N, Alessa A, Mulla R, AlQahtani L, Bajonaid R, Alharthi A, Alnarhdi O, Alanzi N. AI-powered mental health virtual assistants’ acceptance: An empirical study on influencing factors among generations X, Y, and Z. Cureus 15(11): e49486. (2023). doi: 10.7759/cureus.49486\nNo, generative AI continues to propagate existing biases without the safeguards of a clinical provider: Rai S, Stade EC, Giorgi S, Guntuku SC. Key language markers of depression on social media depend on race. Proc Nat Acad Sci 121(14): e2319837121. (2024). doi: 10.1073/pnas.2319837121\n\n\n\n Has your perspective on the role of machine learning and ethical algorithms changed at all since the first module?"
  },
  {
    "objectID": "genai.html#summary",
    "href": "genai.html#summary",
    "title": "Generative AI",
    "section": "Summary",
    "text": "Summary\nGenerative AI is a powerful new software tool that is becoming increasingly popular to use in a variety of different clinical and scientific workflows. While such tools can potentially be used to improve patient care and access to medical resources, they are also associated with their own set of limitations and challenges. As clinicians, we have a responsibility to think critically about the role of generative AI and ensure that future software is used responsibly."
  },
  {
    "objectID": "privacy.html#additional-readings",
    "href": "privacy.html#additional-readings",
    "title": "Privacy and Anonymization",
    "section": "Additional Readings",
    "text": "Additional Readings\n\nGille F, Brall C. Limits of data anonymity: Lack of public awareness risks trust in health system activities. Life Sciences, Society and Policy 17(7). (2021). doi: 10.1186/s40504-021-00115-9\nSavage N. Privacy: The myth of anonymity. Nature 537: S70-2. (2016). doi: 10.1038/537S70a. PMID: 27602747\nKapoor S. Revisiting HIPAA - Privacy concerns in healthcare tech. Berkeley Technology Law Journal. (2023). Link to article\nOhm P. Broken promises of privacy: Responding to the surprising failure of anonymization. UCLA Law Review 57: 1701. (2010). Link to article\nPool J, Akhlaghpour S, Fatehi F, Burton-Jones A. A systematic analysis of failures in protecting personal health data: A scoping review. Int J Inf Manag 74: 102719. (2024). doi: 10.1016/j.ijinfomgt.2023.102719"
  },
  {
    "objectID": "interpretability.html",
    "href": "interpretability.html",
    "title": "Interpretability and Explainability",
    "section": "",
    "text": "TODO\nMade with ❤ by the EAMC Team ©2024."
  },
  {
    "objectID": "interpretability.html#learning-objectives",
    "href": "interpretability.html#learning-objectives",
    "title": "Interpretability and Explainability",
    "section": "",
    "text": "TODO"
  },
  {
    "objectID": "interpretability.html#overview",
    "href": "interpretability.html#overview",
    "title": "Interpretability and Explainability",
    "section": "Overview",
    "text": "Overview"
  },
  {
    "objectID": "interpretability.html#a-detailed-look-hipaa-phi",
    "href": "interpretability.html#a-detailed-look-hipaa-phi",
    "title": "Interpretability and Explainability",
    "section": "A Detailed Look: HIPAA PHI",
    "text": "A Detailed Look: HIPAA PHI\nLet’s take a look at the official list of HIPAA-protected attributes from the Health and Human Services Department:\n\nNames;\nAll geographical subdivisions smaller than a state (e.g., street address, city, county, precinct, ZIP code except for the initial three digits of a ZIP code).1\nAll dates (except year) directly related to an individual (e.g., birth date, admission date, exact ages in years over the ages of 90).2\nPhone numbers, fax numbers, email addresses.\nSocial security numbers, health plan beneficiary numbers, driver license numbers, medical record numbers, etc.\nLicense plates\nIP addresses\nBiometric identifiers (i.e., finger prints, voice recordings, genomic data)\nFull-face photographic images\nAny other unique identifying number, characteristic, or code\n\n1 The initial three digits of a zip code is still considered PHI by HIPAA if the number of individuals residing in all zip codes with those initial three digits is less than 20,000. Why do you think this is the case? How do you think the cutoff of 20,000 individuals was determined?2 Why are ages over 90 years-old considered PHI, but not younger ages?Are there any attributes listed that you didn’t expect? How about attributes that aren’t listed above but should be included?"
  },
  {
    "objectID": "interpretability.html#hands-on-tutorial",
    "href": "interpretability.html#hands-on-tutorial",
    "title": "Interpretability and Explainability",
    "section": "Hands-On Tutorial",
    "text": "Hands-On Tutorial\nFor this exercise, take a look at the following table:\n\n\nTable 1: A Sample Patient Dataset\n\n\nPATIENT_ID\nAGE\nGENDER\nBP\nHIV_STATUS\n\n\n\n\nP001\n45\nM\n120/80\nNegative\n\n\nP002\n60\nF\n135/85\nPositive\n\n\nP003\n33\nM\n128/82\nNegative\n\n\nP004\n50\nF\n142/90\nNegative\n\n\nP005\n27\nM\n110/70\nPositive\n\n\nP006\n38\nF\n125/78\nNegative\n\n\nP007\n55\nM\n138/88\nNegative\n\n\nP008\n43\nF\n132/84\nPositive\n\n\nP009\n29\nM\n118/76\nNegative\n\n\nP010\n61\nF\n145/92\nNegative\n\n\n\n\n\n\n Is Table 1 properly anonymized according to HIPAA regulations?\nImagine that you’re a student working in a research lab and are tasked with analyzing this dataset of patients from the Philadelphia area. Your research mentor tells you that this dataset contains all of the inpatient admissions to HUP from the past week.\nSeparately during your lunch break, you hear on the news that a famous celebrity - a 50 year-old female (in this hypothetical situation) - was recently admitted to HUP last week for a hypertensive crisis, and was just recently discharged from the hospital.33 In case it wasn’t already clear, this dataset and scenario is entirely fictional, and was actually generated using ChatGPT! You can take a look at the generation process here if you’re interested.\n\n\n\n\n\n\nGiven this information, can you identify which patient ID corresponds to the famous celebrity?\n\n\n\n\n\nThe only patient in the table above corresponding to a 50 year-old female with hypertension is patient P004.\n\n\n\nIgnoring the fact that this was a small toy example, how difficult was it to re-identify a patient (namely, the famous celebrity) from the dataset? As a result of the successful re-identification of the patient, were you able to learn anything new about the patient (i.e., take a look at the HIV_STATUS column).\nIt turns out that a very similar re-identification strategy was used by Dr. Latanya Sweeney in 1997 where she successfully re-identified the then Governor of Massachusetts using publicly accessible, anonymized medical records released by the state of Massachusetts.44 Sweeney is an excellent writer and we encourage you to check out two of her publications on this topic: [1] Sweeney L. Only you, your doctor, and many others may know. Tech Sci. (2015). Link to article; [2] Sweeney L. \\(k\\)-Anonymity: A model for protecting privacy. Int J Uncertainty, Fuzziness, and Knowledge-based Systems 10(5): 557-70. (2002). Link to article\nWhy were we and Dr. Sweeney able to re-identify patients from an anonymized dataset? The main reason is that in both situations, we correlated the information in the table with outside knowledge and other datasets in order to gain new, privileged information about patients by synthesizing datasets together. There are countless other examples of re-identifying individuals from anonymized datasets, from identifying Netflix users from anonymized movie ratings to even finally catching the notorious Golden State Killer.\nIn summary, there are two key points that we hope you take away from this exercise:\n\n\n\n\n\n\nProblems with Anonymization\n\n\n\n\nAnonymization is not an effective tool to preserve patient privacy.\nThe reason why anonymization fails is that it assumes there are no other datasets or sources of information in the world to cross-reference."
  },
  {
    "objectID": "interpretability.html#evidence-based-medicine-discussion",
    "href": "interpretability.html#evidence-based-medicine-discussion",
    "title": "Interpretability and Explainability",
    "section": "Evidence-Based Medicine Discussion",
    "text": "Evidence-Based Medicine Discussion\nDo current HIPAA-compliant anonymization standards effectively protect minorities and people of color?\n\nOverview Article: All of Us Research Program Overview. National Institutes of Health. Accessed 19 May 2024. Link to article5\n\n5 There’s a great 2-minute intro video to the All of Us Research program here.\nYes, the anonymization achieved by the All-of-Us dataset is sufficient: Xia W, Basford M, Carroll R, Clayton EW, Harris P, Kantacioglu M, Liu Y, Nyemba S, Vorobeychik Y, Wan Z, Malin BA. Managing re-identification risks while providing access to the All of Us research program. J Am Med Inf Assoc 30(5): 907-14. (2023). doi: 10.1093/jamia/ocad021. PMID: 36809550\nNo, the All-of-Us dataset hurts people of color: Kaiser J. Million-person U.S. study of genes and health stumbles over including Native American groups. Science. (2019). Link to article\n\n\n\n There are other problems involving the All of Us research program, including a recent study inadvertently using “objective” mathematical techniques that inappropriately validates racist and xenophobic ideologies.6 Even well-established data analysis techniques must be used and presented carefully!6 The All of Us Research Program Genomics Investigators. Genomic data in the All of Us research program. Nature 627: 340-6. (2024). doi: 10.1038/s41586-023-06957-x. PMID: 38374255"
  },
  {
    "objectID": "interpretability.html#summary",
    "href": "interpretability.html#summary",
    "title": "Interpretability and Explainability",
    "section": "Summary",
    "text": "Summary\nAnonymization is a common technique used to ensure that publicly released medical datasets are HIPAA-compliant and protect patient identities. Unfortunately, there is a growing body of evidence that shows that anonymization is no longer an effective technique for protecting patient data, and cannot provide any provable guarantees for patient privacy."
  },
  {
    "objectID": "interpretability.html#additional-readings",
    "href": "interpretability.html#additional-readings",
    "title": "Interpretability and Explainability",
    "section": "Additional Readings",
    "text": "Additional Readings\n\nGille F, Brall C. Limits of data anonymity: Lack of public awareness risks trust in health system activities. Life Sciences, Society and Policy 17(7). (2021). doi: 10.1186/s40504-021-00115-9\nSavage N. Privacy: The myth of anonymity. Nature 537: S70-2. (2016). doi: 10.1038/537S70a. PMID: 27602747\nKapoor S. Revisiting HIPAA - Privacy concerns in healthcare tech. Berkeley Technology Law Journal. (2023). Link to article\nOhm P. Broken promises of privacy: Responding to the surprising failure of anonymization. UCLA Law Review 57: 1701. (2010). Link to article\nPool J, Akhlaghpour S, Fatehi F, Burton-Jones A. A systematic analysis of failures in protecting personal health data: A scoping review. Int J Inf Manag 74: 102719. (2024). doi: 10.1016/j.ijinfomgt.2023.102719"
  },
  {
    "objectID": "privacy.html#overview",
    "href": "privacy.html#overview",
    "title": "Privacy and Anonymization",
    "section": "Overview",
    "text": "Overview\nAs clinicians, we deal with patient data every day, and have an ethical (and legal) responsibility to protect patient privacy and confidential information. At the same time, we often work alongside scientists to use patient data to advance our understanding of science. How can we gain meaningful insights from data while still protecting patient identity?\nAccording to the Health Insurance Portability and Accountability Act (HIPAA), one way to accomplish this is through data anonymization. In general, there are two main ways that researchers anonymize data:\n\nCoarsening means we decrease the granularity of the features. For example, using 5-digit zip codes may make it too easy to identify individuals from a dataset, so we might instead choose to coarsen the zip codes by removing the last two digits of each zip code. Instead of including the exact ages of patients, we often coarse the data to only include the decade of the age of the patient.\nReduction means we remove entire features altogether. For example, we might choose to remove all patient names and medical record numbers from a dataset before making it accessible to researchers.\n\nHow can we be certain that a dataset is anonymized “enough”? Formally, a dataset is defined as \\(k\\)-anonymous if there are at least \\(k\\) copies of any given row in a dataset. The concept of \\(k\\)-anonymity is based in the idea of anonymity in numbers - if \\(k\\) is sufficiently large, then it should (hopefully) be impossible to identify any singular individual as a particular row of the dataset because the patient could be any of at least \\(k\\) rows."
  },
  {
    "objectID": "privacy.html#a-detailed-look-hipaa-phi",
    "href": "privacy.html#a-detailed-look-hipaa-phi",
    "title": "Privacy and Anonymization",
    "section": "A Detailed Look: HIPAA PHI",
    "text": "A Detailed Look: HIPAA PHI\nLet’s take a look at the official list of HIPAA-protected attributes from the Health and Human Services Department:\n\nNames;\nAll geographical subdivisions smaller than a state (e.g., street address, city, county, precinct, ZIP code except for the initial three digits of a ZIP code).1\nAll dates (except year) directly related to an individual (e.g., birth date, admission date, exact ages in years over the ages of 90).2\nPhone numbers, fax numbers, email addresses.\nSocial security numbers, health plan beneficiary numbers, driver license numbers, medical record numbers, etc.\nLicense plates\nIP addresses\nBiometric identifiers (i.e., finger prints, voice recordings, genomic data)\nFull-face photographic images\nAny other unique identifying number, characteristic, or code\n\n1 The initial three digits of a zip code is still considered PHI by HIPAA if the number of individuals residing in all zip codes with those initial three digits is less than 20,000. Why do you think this is the case? How do you think the cutoff of 20,000 individuals was determined?2 Why are ages over 90 years-old considered PHI, but not younger ages?Are there any attributes listed that you didn’t expect? How about attributes that aren’t listed above but should be included?"
  },
  {
    "objectID": "privacy.html#hands-on-tutorial",
    "href": "privacy.html#hands-on-tutorial",
    "title": "Privacy and Anonymization",
    "section": "Hands-On Tutorial",
    "text": "Hands-On Tutorial\nFor this exercise, take a look at the following table:\n\n\nTable 1: A Sample Patient Dataset\n\n\nPATIENT_ID\nAGE\nGENDER\nBP\nHIV_STATUS\n\n\n\n\nP001\n45\nM\n120/80\nNegative\n\n\nP002\n60\nF\n135/85\nPositive\n\n\nP003\n33\nM\n128/82\nNegative\n\n\nP004\n50\nF\n142/90\nNegative\n\n\nP005\n27\nM\n110/70\nPositive\n\n\nP006\n38\nF\n125/78\nNegative\n\n\nP007\n55\nM\n138/88\nNegative\n\n\nP008\n43\nF\n132/84\nPositive\n\n\nP009\n29\nM\n118/76\nNegative\n\n\nP010\n61\nF\n145/92\nNegative\n\n\n\n\n\n\n Is Table 1 properly anonymized according to HIPAA regulations?\nImagine that you’re a student working in a research lab and are tasked with analyzing this dataset of patients from the Philadelphia area. Your research mentor tells you that this dataset contains all of the inpatient admissions to HUP from the past week.\nSeparately during your lunch break, you hear on the news that a famous celebrity - a 50 year-old female (in this hypothetical situation) - was recently admitted to HUP last week for a hypertensive crisis, and was just recently discharged from the hospital.33 In case it wasn’t already clear, this dataset and scenario is entirely fictional, and was actually generated using ChatGPT! You can take a look at the generation process here if you’re interested.\n\n\n\n\n\n\nGiven this information, can you identify which patient ID corresponds to the famous celebrity?\n\n\n\n\n\nThe only patient in the table above corresponding to a 50 year-old female with hypertension is patient P004.\n\n\n\nIgnoring the fact that this was a small toy example, how difficult was it to re-identify a patient (namely, the famous celebrity) from the dataset? As a result of the successful re-identification of the patient, were you able to learn anything new about the patient (i.e., take a look at the HIV_STATUS column).\nIt turns out that a very similar re-identification strategy was used by Dr. Latanya Sweeney in 1997 where she successfully re-identified the then Governor of Massachusetts using publicly accessible, anonymized medical records released by the state of Massachusetts.44 Sweeney is an excellent writer and we encourage you to check out two of her publications on this topic: [1] Sweeney L. Only you, your doctor, and many others may know. Tech Sci. (2015). Link to article; [2] Sweeney L. \\(k\\)-Anonymity: A model for protecting privacy. Int J Uncertainty, Fuzziness, and Knowledge-based Systems 10(5): 557-70. (2002). Link to article\nWhy were we and Dr. Sweeney able to re-identify patients from an anonymized dataset? The main reason is that in both situations, we correlated the information in the table with outside knowledge and other datasets in order to gain new, privileged information about patients by synthesizing datasets together. There are countless other examples of re-identifying individuals from anonymized datasets, from identifying Netflix users from anonymized movie ratings to even finally catching the notorious Golden State Killer.\nIn summary, there are two key points that we hope you take away from this exercise:\n\n\n\n\n\n\nProblems with Anonymization\n\n\n\n\nAnonymization is not an effective tool to preserve patient privacy.\nThe reason why anonymization fails is that it assumes there are no other datasets or sources of information in the world to cross-reference."
  },
  {
    "objectID": "privacy.html#evidence-based-medicine-discussion",
    "href": "privacy.html#evidence-based-medicine-discussion",
    "title": "Privacy and Anonymization",
    "section": "Evidence-Based Medicine Discussion",
    "text": "Evidence-Based Medicine Discussion\nDo current HIPAA-compliant anonymization standards effectively protect minorities and people of color?\n\nOverview Article: All of Us Research Program Overview. National Institutes of Health. Accessed 19 May 2024. Link to article5\n\n5 There’s a great 2-minute intro video to the All of Us Research program here.\nYes, the anonymization achieved by the All-of-Us dataset is sufficient: Xia W, Basford M, Carroll R, Clayton EW, Harris P, Kantacioglu M, Liu Y, Nyemba S, Vorobeychik Y, Wan Z, Malin BA. Managing re-identification risks while providing access to the All of Us research program. J Am Med Inf Assoc 30(5): 907-14. (2023). doi: 10.1093/jamia/ocad021. PMID: 36809550\nNo, the All-of-Us dataset hurts people of color: Kaiser J. Million-person U.S. study of genes and health stumbles over including Native American groups. Science. (2019). Link to article\n\n\n\n There are other problems involving the All of Us research program, including a recent study inadvertently using “objective” mathematical techniques that inappropriately validates racist and xenophobic ideologies.6 Even well-established data analysis techniques must be used and presented carefully!6 The All of Us Research Program Genomics Investigators. Genomic data in the All of Us research program. Nature 627: 340-6. (2024). doi: 10.1038/s41586-023-06957-x. PMID: 38374255"
  },
  {
    "objectID": "privacy.html#summary",
    "href": "privacy.html#summary",
    "title": "Privacy and Anonymization",
    "section": "Summary",
    "text": "Summary\nAnonymization is a common technique used to ensure that publicly released medical datasets are HIPAA-compliant and protect patient identities. Unfortunately, there is a growing body of evidence that shows that anonymization is no longer an effective technique for protecting patient data, and cannot provide any provable guarantees for patient privacy. At the end of the day, robustly guaranteeing patient privacy is a difficult task and requires conscious efforts from both clinicians and researchers alike."
  }
]